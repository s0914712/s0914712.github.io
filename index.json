[{"categories":["History and Taiwan"],"contents":" ##歷史典故：科西拉、科林斯與雅典 在古希臘世界的漫長歷史長河中，有一個關於權力、野心和命運的故事，被記錄在修昔底德的《伯羅奔尼撒戰爭史》中，這個故事猶如一面映照現代國際政治的明鏡。 科西拉，這個位於愛奧尼亞海的小島，原本只是科林斯強大帝國的一個邊陲殖民地。母邦與殖民地之間的關係如同君主與臣服領地，看似穩固，實則暗潮洶湧。科西拉憑藉其得天獨厚的地理位置和日益強大的海軍實力，逐漸萌生了掙脫科林斯控制的野心。 這是一個關於權力平衡的永恒故事。科西拉並非一味盲目反抗，而是在國際政治的棋盤上審慎地計算著每一步。她意識到單憑自身力量難以完全擺脫科林斯的陰影，於是將目光投向了當時正在崛起的雅典。這一戰略聯盟不僅是尋求生存之道，更是一場精心設計的政治算計。 雅典，這個以民主和文明著稱的城邦，並非出於純粹的慈善，而是敏銳地捕捉到了一個牽制科林斯的絕佳機會。兩個城邦的結盟，表面上是保護與被保護的關係，實則是一場複雜的政治角力。雅典的軍事支持不僅是對科西拉的救贖，更是對科林斯實力的一記重擊。 科林斯如何看待這一切？作為一個曾經的霸主，他們視科西拉的背叛如同對尊嚴的赤裸裸侮辱。這種背叛觸及的不僅是領土，更是一個帝國的榮耀與威信。科林斯被迫做出回應，他們明白，縱容科西拉的行為，意味著帝國影響力的削弱。 這場看似局部的衝突，實則埋下了伯羅奔尼撒戰爭的種子。兩個強權——雅典與科林斯，因為一個小島的命運而最終走向全面對抗。這不僅是一場軍事衝突，更是一場關於主權、影響力和國際秩序的根本性較量。 在古希臘的世界中，有一個小島科西拉和一個强大的城邦科林斯之間的故事，彷彿在訴說著當今台灣與中國之間的複雜命運。\n##想像一下 科西拉就像台灣，一個渴望自主卻又備受威脅的小島。她不想被科林斯完全控制，但又無法完全獨立。自1949年以來，台灣一直在這樣的夾縫中求生存，一邊鞏固著自己的民主制度，一邊小心翼翼地平衡著來自強大鄰國的壓力。 科西拉知道自己需要盟友，於是她轉向強大的雅典——就像台灣投向美國的懷抱。這不僅僅是尋求保護，更是一場複雜的戰略遊戲。美國支持台灣，不僅因為關心這個民主社會，更是為了牽制中國在亞太地區不斷擴張的野心。 而科林斯，正如今天的中國，對科西拉的抗爭怒不可遏。在他們眼中，這不僅僅是一個小島的反抗，更是對自身權威的赤裸裸挑釁。中國堅稱台灣是其不可分割的一部分，並且不排除使用武力來\u0026quot;收復\u0026quot;這片土地。 這是一個跨越千年的政治寓言。無論是古希臘的海域，還是現代的台灣海峽，強權與自由、控制與抵抗的鬥爭始終在上演。科西拉的故事告訴我們，小國的命運往往懸而未決，取決於國際局勢的微妙平衡。 台灣就像科西拉，在強權夾縫中堅韌地生存。她不僅在抵抗，更在證明自己存在的價值。無論是經濟、科技還是民主，台灣都在向世界證明：即便是小島，也可以擁有自己的尊嚴和選擇。 這個故事還在繼續，而結局，有待書寫。 2. 歷史上的理性與傲慢：伯羅奔尼撒戰爭與現代戰爭 伯羅奔尼撒戰爭中的理性失敗：\n修昔底德記錄了伯里克利的策略失誤。他認為支持科西拉對抗科林斯可以增強雅典的實力，但卻未預料到斯巴達的參與及長期消耗戰的後果。 伯里克利的推演過於依賴理性設計的“完美計劃”，忽略了人性、情緒和偶然因素，最終引發了雅典的災難。 現代戰爭的平行例子：\n德國在1914年的戰略設計類似雅典的情況。他們預計俄羅斯會保持中立，或者若干替代方案會奏效。但實際上，所有假設都錯誤，並導致第一次世界大戰的爆發。 修昔底德提醒，理性在制定複雜策略時，可能忽視意料之外的情況和人性中的不可預測性，從而引發悲劇。\n我們學習國際關係試圖預測對手的策略，但得到的答案是我們無法預測\n","permalink":"https://s0914712.github.io/blog/post-5/","tags":["Thucydides Trap","history","Trap","Taiwan"],"title":"修昔底德的陷阱(Thucydides Trap)"},{"categories":["History and Taiwan"],"contents":" 兵棋推演背後的核威脅：中美對峙與臺灣議題的新思考 2024 年 12 月 13 日，華盛頓智庫 CSIS 公布了一份名為《面對世界末日的核威懾》的兵棋推演報告，探討中美於臺灣衝突情境下使用核武與外交手段的可能後果。不少媒體過度強調「誰勝誰負」，忽略了這場研究真正的重點：核戰與外交決策的風險。\n一、CSIS 兵棋推演：重點與意義 1. 沒有「臺灣隊」，而是聚焦核武策略 在這場兵棋推演中，臺灣並未被設置成「隊伍」登場，且研究者在設定中明確指出「臺灣不會中途投降」。因此，模擬的重點不是「臺灣能否抵抗多久」，而是美中雙方在衝突中是否以及如何使用核武與外交手段。\n2. 研究結果：未有絕對勝者 報告指出，若雙方持續升級核衝突，最終往往以「兩敗俱傷」收場。若一方願意讓步、提供理性下台階，則衝突可能趨於和緩。這也呼應了研究的核心：探討如何避免核災難，而非聚焦軍事兵棋推演的登陸時間或傷亡規模。\n3. 啟示：核武作為最後手段的威脅 兵棋推演顯示，一旦某方在戰場遭遇重大挫敗，即可能考慮使用核武。若雙方互不退讓，局勢恐進一步升級，最終導致毀滅性後果。\n二、臺灣媒體解讀：為何會失焦？ 1. 標題簡化與新聞效應 兵棋推演的複雜情境常被簡化為「幾勝幾敗」的二分法，雖能吸引讀者注意，但忽略了核武風險與外交博弈的深層意義。當媒體報導的焦點僅在「輸贏」，讀者便容易忽視核戰風險的防範與因應。\n2. 核議題報導的挑戰 由於地緣關係，臺灣媒體對國際軍事動態高度敏感；但兵棋推演與核武議題的深度分析需要時間與專業知識。CSIS 的英文報告達 200 多頁，若為了搶流量與時效，媒體可能僅截取部分重點甚至以偏概全，造成焦點偏移。\n三、全球核武庫擴張下的因應之道 根據 2024 年 6 月瑞典斯德哥爾摩國際和平研究所（SIPRI）的年度報告，估計中國核武庫的彈頭數量已從 2023 年 1 月的 410 枚增至 2024 年 1 月的 500 枚。\n核武數量的背後含義：不僅是核彈頭的增長，更突顯出大國間戰略影響力的競逐。 應對之道：美國學者呼籲重新評估核武部署，同時推動多邊或雙邊機制管控核武發展，降低擦槍走火風險。如同古巴危機時的經驗，提供「下台階」是避免全面衝突的關鍵。 四、兵棋推演結果分析 結果概況 美方隊伍 中方隊伍 最後結果 發生次數 美方始終保持冷靜（局勢美方有利） 中方冷靜避免衝突 中方最終認輸 → 美方勝 4 美方始終保持冷靜（局勢美方有利） 中方使用 EMP，升高局勢 美方勝 1 美方始終保持冷靜（局勢美方有利） 中方使用 EMP，但美方不理會 美方勝 1 戰況膠著（美方兩次使用 EMP 彈） 中方率先使用核武 1. 美方使用核武 → 回到戰前 1 2. 美方對設施核攻擊 → 兩敗俱傷 3 3. 美方少量核武反制軍事基地 → 中方勝 4 美方一開始挫敗 中方冷靜 中方勝 1 結果分析 中方對臺灣及美國在亞太的基地進行核攻擊。 美方隨後對中方軍事或民用設施進行核報復。 最終，多數模擬結果導向災難性結局。 五、結論與呼籲 中美兩強的核態勢持續升溫，臺海問題被視為一大導火線。然而，正如這次兵棋推演所揭示的，當核武成為博弈手段時，勝利已經變得模糊不清。任何一方的絕對勝利都將帶來難以彌補的浩劫。\n決策者與媒體責任：思考如何避免核戰的風險。是繼續比拚火力，還是尋求更多外交與對話的可能性？ 參考資料 Cancian, Mark, Matthew Cancian, and Eric Heginbotham. Confronting Armageddon: Wargaming Nuclear Deterrence and Its Failures in a U.S.–China Conflict over Taiwan. Washington, DC: Center for Strategic and International Studies (CSIS), December 2024. 閱讀報告 Stockholm International Peace Research Institute (SIPRI). Role of Nuclear Weapons Grows as Geopolitical Relations Deteriorate—New SIPRI Yearbook Out Now. Press Release, June 17, 2024. 閱讀報告 Matthew Kroenig. The Logic of American Nuclear Strategy: Why Strategic Superiority Matters. New York: Oxford Academic Books, 2018. ","permalink":"https://s0914712.github.io/blog/post-6/","tags":["兵推 WarGame","history","CSIS","Taiwan"],"title":"兵棋推演背後的核威脅：中美對峙與臺灣議題的新思考"},{"categories":["AI解決軍事問題"],"contents":" 長短期記憶神經網路 當我們在理解一件事情的時候，通常不會每次都從頭開始學習，而是透過既有的知識與記憶來理解當下遇到的問題；事件的發生通常具有連續性，也就是一連串的因果關係，或是一個持續不斷變動的結果。在機器學習模型的發展中，引入這種遞歸 (recurrent) 的概念，是遞歸神經網路與其他神經網路模型 (如 CNN) 相比，較為創新的地方。長短期記憶模型則是改善了遞歸神經網路在長期記憶上的一些不足，因為其強大的辨識能力，可以有效的對上下文產生連結，現在已大量運用在自然語言理解 (例如語音轉文字，翻譯，產生手寫文字)，圖像與影像辨識等應用。 在這邊我先不介紹LSTM的原理，因為這個專案並不是企圖改進LSTM的架構，大家只要知道他是具有理解時間關係的網路結構就可以了，既然prophet 和LSTM 都可以進行時間序列的處理 那我們就使用這兩個做個比較。\nMeta Prophet 簡介：重點在於如何處理時間 Meta Prophet（全名：Facebook Prophet）是一款專門用於時間序列預測的開源工具，由 Facebook（Meta）開發。它以 Python 和 R 實作，設計目標是讓非專業統計人員也能輕鬆處理帶有季節性與節慶效應的時間序列數據。Prophet 被廣泛應用在營收、網站流量、銷售、需求等數據預測領域。\nProphet 的核心概念與架構 Prophet 將時間序列建模公式拆成三大部分： [ y(t) = g(t) + s(t) + h(t) + \\epsilon_t ]\ng(t)：trend（趨勢） s(t)：seasonality（季節性成分） h(t)：holidays（節日或特殊事件） ε：雜訊 Prophet 處理時間的關鍵特點 1. 自動識別「時間」欄位 Prophet 要求資料有兩個欄位：\nds（datestamp，日期/時間） y（數值） 它會自動將 ds 欄位辨識為時間軸，並用它進行所有的分解與運算。\n2. 趨勢（Trend）建模 Prophet 支持兩種主要趨勢建模方式：\n線性趨勢（可設定 changepoints 轉折點） logistic 增長（適合有上限的數據，如市場飽和） Prophet 會在時間軸上自動尋找可能出現轉折（changepoints）的時刻，允許趨勢在不同時期改變成長率。\n3. 週期/季節性（Seasonality） Prophet 會自動加入年、週、日等週期性成分，透過傅立葉級數進行建模。你可以手動新增更多週期（如每月），或調整週期的強度。\n4. 節日/事件 你可以自訂「節日」時間清單（如農曆年、雙 11 等），Prophet 會自動在這些時間點上套用額外的預測效果（如特定天數的異常波動）。\n5. 處理缺漏值與不規則間距 Prophet 可直接處理缺漏資料與不等間隔的時間序列，不需手動補齊時間軸。\n6. 時間格式彈性 支援多種時間格式（日期、日期時間、timestamp），Prophet 會自動解析。\n7. 可擴展未來的時間軸 你只需告訴 Prophet 要預測幾天/幾週/幾個月後的數值，它會自動根據 trend 和 seasonality 外推時間序列。\nProphet 處理時間序列的步驟（Python 範例） import pandas as pd from prophet import Prophet # 資料格式要求 # df 必須包含兩個欄位：ds（日期），y（數值） df = pd.DataFrame({ \u0026#39;ds\u0026#39;: pd.date_range(\u0026#39;2023-01-01\u0026#39;, periods=100), \u0026#39;y\u0026#39;: [ ... ] # 你的數值 }) # 建立模型 m = Prophet() m.fit(df) # 預測未來 30 天 future = m.make_future_dataframe(periods=30) forecast = m.predict(future) # 繪圖 fig = m.plot(forecast) Prophet vs. LSTM：時間處理方式比較總結 處理特點 Prophet LSTM 1. 時間格式解析 自動辨識 ds 欄位的時間格式（日期、datetime、timestamp 等） 時間需手動轉換為連續數值或標準化後的時間特徵 2. 趨勢、季節性建模 內建趨勢與多重季節性建模邏輯（如年、週、日）可外加節日等事件效應 不會自動建模趨勢與季節性，需透過大量訓練數據學出時間模式 3. 缺漏資料處理 可直接處理不連續與缺漏資料，不需補齊時間軸 缺值需事先補齊，並要求固定間距的時間序列輸入 4. 未來預測方式 明確定義未來時間點，自動產生未來時間序列並套用模型外推 需遞迴地逐步預測下一時間點，常使用前一步預測作為下一步輸入 5. 時間粒度調整 可輕鬆切換日、週、月等粒度 時間粒度需在特徵工程階段預先設計 6. 複雜性與需求 適合小資料量、具明顯週期性與事件驅動的商業場景 適合大量數據、複雜長期依賴問題（如語音、股價等非週期性應用） 算出來結果差不多 ##下面實戰 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34;使用PROPHET 以及LSTM預測總架次數.ipynb Automatically generated by Colab. Original file is located at https://colab.research.google.com/drive/1rRZQvGW2IOwRO1i1FZd_wGhz5iagkko6 \u0026#34;\u0026#34;\u0026#34; # @title # 設定分割日 mday mday = pd.to_datetime(\u0026#39;2023-10-1\u0026#39;) # 建立訓練用 index 與驗證用 index train_index = df2[\u0026#39;ds\u0026#39;] \u0026lt; mday test_index = df2[\u0026#39;ds\u0026#39;] \u0026gt;= mday # 分割輸入資料 x_train = df2[train_index] x_test = df2[test_index] # 分割日期資料（用於繪製圖形） dates_test = df2[\u0026#39;ds\u0026#39;][test_index] \u0026#34;\u0026#34;\u0026#34;### Read DATA\u0026#34;\u0026#34;\u0026#34; import pandas as pd # 直接使用CSV檔案的URL url = \u0026#39;https://docs.google.com/spreadsheets/d/1hkHrnbn5oQPPHfBhiuieUzmVjsqW20XmRQWkhfWMomE/export?format=csv\u0026amp;id=1hkHrnbn5oQPPHfBhiuieUzmVjsqW20XmRQWkhfWMomE\u0026#39; # 使用pandas讀取CSV df = pd.read_csv(url) # 顯示DataFrame的前幾行 df.head() df.to_csv(\u0026#39;calendar.csv\u0026#39;, index=False) import pandas as pd # 讀取原始 CSV 檔案 df = pd.read_csv(\u0026#39;/content/calender.csv\u0026#39;) # 確保日期格式正確 df[\u0026#39;ds\u0026#39;] = pd.to_datetime(df[\u0026#39;pla_aircraft_sorties\u0026#39;]) # 可以在這裡進行轉換，例如改欄位名稱、格式等 df.rename(columns={\u0026#39;pla_aircraft_sorties\u0026#39;: \u0026#39;value\u0026#39;}, inplace=True) # 轉為 Prophet 格式範例 df.rename(columns={\u0026#39;date\u0026#39;: \u0026#39;DATE\u0026#39;}, inplace=True) # 轉為 Prophet 格式範例 # 儲存為新的 CSV 檔案 df.to_csv(\u0026#39;/content/output.csv\u0026#39;, index=False) import pandas as pd from datetime import datetime def convert_for_prophet(input_file, output_file): df = pd.read_csv(input_file) df.columns = df.columns.str.strip() df[\u0026#39;DATE\u0026#39;] = pd.to_datetime(df[\u0026#39;DATE\u0026#39;]) prophet_df = pd.DataFrame() prophet_df[\u0026#39;ds\u0026#39;] = df[\u0026#39;DATE\u0026#39;] prophet_df[\u0026#39;value\u0026#39;]=df[\u0026#39;value\u0026#39;] prophet_df[\u0026#39;ds\u0026#39;] = prophet_df[\u0026#39;ds\u0026#39;].dt.strftime(\u0026#39;%Y-%m-%d\u0026#39;) prophet_df = prophet_df.sort_values(\u0026#39;ds\u0026#39;) prophet_df.to_csv(output_file, index=False) print(f\u0026#34;\\n Save File {output_file}\u0026#34;) print(\u0026#34;\\n Preview：\u0026#34;) print(prophet_df.head()) return prophet_df input_file = \u0026#39;output.csv\u0026#39; output_file = \u0026#39;DEXVZUS_out.csv\u0026#39; prophet_df = convert_for_prophet(input_file, output_file) !pip install prophet # Quandl for financial analysis, pandas and numpy for data manipulation # fbprophet for additive models, #pytrends for Google trend data import pandas as pd import numpy as np import prophet # matplotlib pyplot for plotting import matplotlib.pyplot as plt import matplotlib # Class for analyzing and (attempting) to predict future prices # Contains a number of visualizations and analysis methods class Stocker(): # Initialization requires a ticker symbol def __init__(self, price): self.symbol = \u0026#39;the \u0026#39; s = price stock = pd.DataFrame({\u0026#39;Date\u0026#39;:s.index, \u0026#39;y\u0026#39;:s, \u0026#39;ds\u0026#39;:s.index, \u0026#39;close\u0026#39;:s,\u0026#39;open\u0026#39;:s}, index=None) if (\u0026#39;Adj. Close\u0026#39; not in stock.columns): stock[\u0026#39;Adj. Close\u0026#39;] = stock[\u0026#39;close\u0026#39;] stock[\u0026#39;Adj. Open\u0026#39;] = stock[\u0026#39;open\u0026#39;] stock[\u0026#39;y\u0026#39;] = stock[\u0026#39;Adj. Close\u0026#39;] stock[\u0026#39;Daily Change\u0026#39;] = stock[\u0026#39;Adj. Close\u0026#39;] - stock[\u0026#39;Adj. Open\u0026#39;] # Data assigned as class attribute self.stock = stock.copy() # Minimum and maximum date in range self.min_date = min(stock[\u0026#39;ds\u0026#39;]) self.max_date = max(stock[\u0026#39;ds\u0026#39;]) # Find max and min prices and dates on which they occurred self.max_price = np.max(self.stock[\u0026#39;y\u0026#39;]) self.min_price = np.min(self.stock[\u0026#39;y\u0026#39;]) self.min_price_date = self.stock[self.stock[\u0026#39;y\u0026#39;] == self.min_price][\u0026#39;ds\u0026#39;] self.min_price_date = self.min_price_date[self.min_price_date.index[0]] self.max_price_date = self.stock[self.stock[\u0026#39;y\u0026#39;] == self.max_price][\u0026#39;ds\u0026#39;] self.max_price_date = self.max_price_date[self.max_price_date.index[0]] # The starting price (starting with the opening price) self.starting_price = float(self.stock[\u0026#39;Adj. Open\u0026#39;].iloc[0]) # The most recent price self.most_recent_price = float(self.stock[\u0026#39;y\u0026#39;].iloc[len(self.stock) - 1]) # Whether or not to round dates self.round_dates = True # Number of years of data to train on self.training_years = 3 # Prophet parameters # Default prior from library self.changepoint_prior_scale = 0.05 self.weekly_seasonality = False self.daily_seasonality = False self.monthly_seasonality = True self.yearly_seasonality = True self.changepoints = None print(\u0026#39;{} Stocker Initialized. Data covers {} to {}.\u0026#39;.format(self.symbol, self.min_date, self.max_date)) \u0026#34;\u0026#34;\u0026#34; Make sure start and end dates are in the range and can be converted to pandas datetimes. Returns dates in the correct format \u0026#34;\u0026#34;\u0026#34; def handle_dates(self, start_date, end_date): # Default start and end date are the beginning and end of data if start_date is None: start_date = self.min_date if end_date is None: end_date = self.max_date try: # Convert to pandas datetime for indexing dataframe start_date = pd.to_datetime(start_date) end_date = pd.to_datetime(end_date) except Exception as e: print(\u0026#39;Enter valid pandas date format.\u0026#39;) print(e) return valid_start = False valid_end = False # User will continue to enter dates until valid dates are met while (not valid_start) \u0026amp; (not valid_end): valid_end = True valid_start = True if end_date \u0026lt; start_date: print(\u0026#39;End Date must be later than start date.\u0026#39;) start_date = pd.to_datetime(input(\u0026#39;Enter a new start date: \u0026#39;)) end_date= pd.to_datetime(input(\u0026#39;Enter a new end date: \u0026#39;)) valid_end = False valid_start = False else: if end_date \u0026gt; self.max_date: print(\u0026#39;End Date exceeds data range\u0026#39;) end_date= pd.to_datetime(input(\u0026#39;Enter a new end date: \u0026#39;)) valid_end = False if start_date \u0026lt; self.min_date: print(\u0026#39;Start Date is before date range\u0026#39;) start_date = pd.to_datetime(input(\u0026#39;Enter a new start date: \u0026#39;)) valid_start = False return start_date, end_date \u0026#34;\u0026#34;\u0026#34; Return the dataframe trimmed to the specified range. \u0026#34;\u0026#34;\u0026#34; def make_df(self, start_date, end_date, df=None): # Default is to use the object stock data if not df: df = self.stock.copy() start_date, end_date = self.handle_dates(start_date, end_date) # keep track of whether the start and end dates are in the data start_in = True end_in = True # If user wants to round dates (default behavior) if self.round_dates: # Record if start and end date are in df if (start_date not in list(df[\u0026#39;Date\u0026#39;])): start_in = False if (end_date not in list(df[\u0026#39;Date\u0026#39;])): end_in = False # If both are not in dataframe, round both if (not end_in) \u0026amp; (not start_in): trim_df = df[(df[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (df[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] else: # If both are in dataframe, round neither if (end_in) \u0026amp; (start_in): trim_df = df[(df[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (df[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] else: # If only start is missing, round start if (not start_in): trim_df = df[(df[\u0026#39;Date\u0026#39;] \u0026gt; start_date) \u0026amp; (df[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] # If only end is imssing round end elif (not end_in): trim_df = df[(df[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (df[\u0026#39;Date\u0026#39;] \u0026lt; end_date)] else: valid_start = False valid_end = False while (not valid_start) \u0026amp; (not valid_end): start_date, end_date = self.handle_dates(start_date, end_date) # No round dates, if either data not in, print message and return if (start_date in list(df[\u0026#39;Date\u0026#39;])): valid_start = True if (end_date in list(df[\u0026#39;Date\u0026#39;])): valid_end = True # Check to make sure dates are in the data if (start_date not in list(df[\u0026#39;Date\u0026#39;])): print(\u0026#39;Start Date not in data (either out of range or not a trading day.)\u0026#39;) start_date = pd.to_datetime(input(prompt=\u0026#39;Enter a new start date: \u0026#39;)) elif (end_date not in list(df[\u0026#39;Date\u0026#39;])): print(\u0026#39;End Date not in data (either out of range or not a trading day.)\u0026#39;) end_date = pd.to_datetime(input(prompt=\u0026#39;Enter a new end date: \u0026#39;) ) # Dates are not rounded trim_df = df[(df[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (df[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] return trim_df # Basic Historical Plots and Basic Statistics def plot_stock(self, start_date=None, end_date=None, stats=[\u0026#39;Adj. Close\u0026#39;], plot_type=\u0026#39;basic\u0026#39;): self.reset_plot() if start_date is None: start_date = self.min_date if end_date is None: end_date = self.max_date stock_plot = self.make_df(start_date, end_date) colors = [\u0026#39;r\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;m\u0026#39;] for i, stat in enumerate(stats): stat_min = min(stock_plot[stat]) stat_max = max(stock_plot[stat]) stat_avg = np.mean(stock_plot[stat]) date_stat_min = stock_plot[stock_plot[stat] == stat_min][\u0026#39;Date\u0026#39;] date_stat_min = date_stat_min[date_stat_min.index[0]] date_stat_max = stock_plot[stock_plot[stat] == stat_max][\u0026#39;Date\u0026#39;] date_stat_max = date_stat_max[date_stat_max.index[0]] print(\u0026#39;Maximum {} = {:.2f} on {}.\u0026#39;.format(stat, stat_max, date_stat_max)) print(\u0026#39;Minimum {} = {:.2f} on {}.\u0026#39;.format(stat, stat_min, date_stat_min)) print(\u0026#39;Current {} = {:.2f} on {}.\\n\u0026#39;.format(stat, self.stock[stat].iloc[len(self.stock) - 1], self.max_date.date())) # Percentage y-axis if plot_type == \u0026#39;pct\u0026#39;: # Simple Plot plt.style.use(\u0026#39;fivethirtyeight\u0026#39;); if stat == \u0026#39;Daily Change\u0026#39;: plt.plot(stock_plot[\u0026#39;Date\u0026#39;], 100 * stock_plot[stat], color = colors[i], linewidth = 2.4, alpha = 0.9, label = stat) else: plt.plot(stock_plot[\u0026#39;Date\u0026#39;], 100 * (stock_plot[stat] - stat_avg) / stat_avg, color = colors[i], linewidth = 2.4, alpha = 0.9, label = stat) plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Change Relative to Average (%)\u0026#39;); plt.title(\u0026#39;%s Currency History\u0026#39; % self.symbol); plt.legend(prop={\u0026#39;size\u0026#39;:10}) plt.grid(color = \u0026#39;k\u0026#39;, alpha = 0.4); # Stat y-axis elif plot_type == \u0026#39;basic\u0026#39;: plt.style.use(\u0026#39;fivethirtyeight\u0026#39;); plt.plot(stock_plot[\u0026#39;Date\u0026#39;], stock_plot[stat], color = colors[i], linewidth = 3, label = stat, alpha = 0.8) plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;US $\u0026#39;); plt.title(\u0026#39;%s Currency History\u0026#39; % self.symbol); plt.legend(prop={\u0026#39;size\u0026#39;:10}) plt.grid(color = \u0026#39;k\u0026#39;, alpha = 0.4); plt.show(); # Reset the plotting parameters to clear style formatting # Not sure if this should be a static method @staticmethod def reset_plot(): # Restore default parameters matplotlib.rcParams.update(matplotlib.rcParamsDefault) # Adjust a few parameters to liking matplotlib.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8, 5) matplotlib.rcParams[\u0026#39;axes.labelsize\u0026#39;] = 10 matplotlib.rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 8 matplotlib.rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 8 matplotlib.rcParams[\u0026#39;axes.titlesize\u0026#39;] = 14 matplotlib.rcParams[\u0026#39;text.color\u0026#39;] = \u0026#39;k\u0026#39; # Method to linearly interpolate prices on the weekends def resample(self, dataframe): # Change the index and resample at daily level dataframe = dataframe.set_index(\u0026#39;ds\u0026#39;) dataframe = dataframe.resample(\u0026#39;D\u0026#39;) # Reset the index and interpolate nan values dataframe = dataframe.reset_index(level=0) dataframe = dataframe.interpolate() return dataframe # Remove weekends from a dataframe def remove_weekends(self, dataframe): # Reset index to use ix dataframe = dataframe.reset_index(drop=True) weekends = [] # Find all of the weekends for i, date in enumerate(dataframe[\u0026#39;ds\u0026#39;]): if (date.weekday()) == 5 | (date.weekday() == 6): weekends.append(i) # Drop the weekends dataframe = dataframe.drop(weekends, axis=0) return dataframe # Calculate and plot profit from buying and holding shares for specified date range def buy_and_hold(self, start_date=None, end_date=None, nshares=1): self.reset_plot() start_date, end_date = self.handle_dates(start_date, end_date) # Find starting and ending price of stock start_price = float(self.stock[self.stock[\u0026#39;Date\u0026#39;] == start_date][\u0026#39;Adj. Open\u0026#39;]) end_price = float(self.stock[self.stock[\u0026#39;Date\u0026#39;] == end_date][\u0026#39;Adj. Close\u0026#39;]) # Make a profit dataframe and calculate profit column profits = self.make_df(start_date, end_date) profits[\u0026#39;hold_profit\u0026#39;] = nshares * (profits[\u0026#39;Adj. Close\u0026#39;] - start_price) # Total profit total_hold_profit = nshares * (end_price - start_price) print(\u0026#39;{} Total buy and hold profit from {} to {} for {} shares = ${:.2f}\u0026#39;.format (self.symbol, start_date, end_date, nshares, total_hold_profit)) # Plot the total profits plt.style.use(\u0026#39;dark_background\u0026#39;) # Location for number of profit text_location = (end_date - pd.DateOffset(months = 1)) # Plot the profits over time plt.plot(profits[\u0026#39;Date\u0026#39;], profits[\u0026#39;hold_profit\u0026#39;], \u0026#39;b\u0026#39;, linewidth = 3) plt.ylabel(\u0026#39;Profit ($)\u0026#39;); plt.xlabel(\u0026#39;Date\u0026#39;); plt.title(\u0026#39;Buy and Hold Profits for {} {} to {}\u0026#39;.format( self.symbol, start_date, end_date)) # Display final value on graph plt.text(x = text_location, y = total_hold_profit + (total_hold_profit / 40), s = \u0026#39;$%d\u0026#39; % total_hold_profit, color = \u0026#39;g\u0026#39; if total_hold_profit \u0026gt; 0 else \u0026#39;r\u0026#39;, size = 14) plt.grid(alpha=0.2) plt.show(); # Create a prophet model without training def create_model(self): # Make the model model = prophet.Prophet(daily_seasonality=self.daily_seasonality, weekly_seasonality=self.weekly_seasonality, yearly_seasonality=self.yearly_seasonality, changepoint_prior_scale=self.changepoint_prior_scale, changepoints=self.changepoints) if self.monthly_seasonality: # Add monthly seasonality model.add_seasonality(name = \u0026#39;monthly\u0026#39;, period = 30.5, fourier_order = 5) return model # Graph the effects of altering the changepoint prior scale (cps) def changepoint_prior_analysis(self, changepoint_priors=[0.001, 0.05, 0.1, 0.2], colors=[\u0026#39;b\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;grey\u0026#39;, \u0026#39;gold\u0026#39;]): # Training and plotting with specified years of data train = self.stock[(self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (max(self.stock[\u0026#39;Date\u0026#39;] ) - pd.DateOffset(years=self.training_years)))] # Iterate through all the changepoints and make models for i, prior in enumerate(changepoint_priors): # Select the changepoint self.changepoint_prior_scale = prior # Create and train a model with the specified cps model = self.create_model() model.fit(train) future = model.make_future_dataframe(periods=180, freq=\u0026#39;D\u0026#39;) # Make a dataframe to hold predictions if i == 0: predictions = future.copy() future = model.predict(future) # Fill in prediction dataframe predictions[\u0026#39;%.3f_yhat_upper\u0026#39; % prior] = future[\u0026#39;yhat_upper\u0026#39;] predictions[\u0026#39;%.3f_yhat_lower\u0026#39; % prior] = future[\u0026#39;yhat_lower\u0026#39;] predictions[\u0026#39;%.3f_yhat\u0026#39; % prior] = future[\u0026#39;yhat\u0026#39;] # Remove the weekends predictions = self.remove_weekends(predictions) # Plot set-up self.reset_plot() plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) fig, ax = plt.subplots(1, 1) # Actual observations ax.plot(train[\u0026#39;ds\u0026#39;], train[\u0026#39;y\u0026#39;], \u0026#39;ko\u0026#39;, ms = 4, label = \u0026#39;Observations\u0026#39;) color_dict = {prior: color for prior, color in zip(changepoint_priors, colors)} # Plot each of the changepoint predictions for prior in changepoint_priors: # Plot the predictions themselves ax.plot(predictions[\u0026#39;ds\u0026#39;], predictions[\u0026#39;%.3f_yhat\u0026#39; % prior], linewidth = 1.2, color = color_dict[prior], label = \u0026#39;%.3f prior scale\u0026#39; % prior) # Plot the uncertainty interval ax.fill_between(predictions[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), predictions[\u0026#39;%.3f_yhat_upper\u0026#39; % prior], predictions[\u0026#39;%.3f_yhat_lower\u0026#39; % prior], facecolor = color_dict[prior], alpha = 0.3, edgecolor = \u0026#39;k\u0026#39;, linewidth = 0.6) # Plot labels plt.legend(loc = 2, prop={\u0026#39;size\u0026#39;: 10}) plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Stock Price ($)\u0026#39;); plt.title(\u0026#39;Effect of Changepoint Prior Scale\u0026#39;); plt.show() # Basic prophet model for specified number of days def create_prophet_model(self, days=0, resample=False): self.reset_plot() model = self.create_model() # Fit on the stock history for self.training_years number of years stock_history = self.stock[self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (self.max_date - pd.DateOffset(years = self.training_years))] if resample: stock_history = self.resample(stock_history) model.fit(stock_history) # Make and predict for next year with future dataframe future = model.make_future_dataframe(periods = days, freq=\u0026#39;D\u0026#39;) future = model.predict(future) if days \u0026gt; 0: # Print the predicted price predicted_date = future[\u0026#39;ds\u0026#39;].iloc[-1] predicted_value = future[\u0026#39;yhat\u0026#39;].iloc[-1] print(\u0026#39;Predicted Index on {} = ${:.2f}\u0026#39;.format(predicted_date, predicted_value)) title = \u0026#39;%s Historical and Predicted bolivars Index\u0026#39; % self.symbol else: title = \u0026#39;%s Historical and Modeled bolivars Index\u0026#39; % self.symbol # Set up the plot fig, ax = plt.subplots(1, 1) # Plot the actual values ax.plot(stock_history[\u0026#39;ds\u0026#39;], stock_history[\u0026#39;y\u0026#39;], \u0026#39;ko-\u0026#39;, linewidth = 1.4, alpha = 0.8, ms = 1.8, label = \u0026#39;Observations\u0026#39;) # Plot the predicted values ax.plot(future[\u0026#39;ds\u0026#39;], future[\u0026#39;yhat\u0026#39;], \u0026#39;forestgreen\u0026#39;,linewidth = 2.4, label = \u0026#39;Modeled\u0026#39;); # Plot the uncertainty interval as ribbon ax.fill_between(np.array(future[\u0026#39;ds\u0026#39;].dt.to_pydatetime()), future[\u0026#39;yhat_upper\u0026#39;], future[\u0026#39;yhat_lower\u0026#39;], alpha=0.3, facecolor=\u0026#39;g\u0026#39;, edgecolor=\u0026#39;k\u0026#39;, linewidth=1.4) # Plot formatting plt.legend(loc = 2, prop={\u0026#39;size\u0026#39;: 10}); plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Index $\u0026#39;); plt.grid(linewidth=0.6, alpha = 0.6) plt.title(title); plt.show() return model, future # Evaluate prediction model for one year def evaluate_prediction(self, start_date=None, end_date=None, nshares = None): # Default start date is one year before end of data # Default end date is end date of data if start_date is None: start_date = self.max_date - pd.DateOffset(years=1) if end_date is None: end_date = self.max_date start_date, end_date = self.handle_dates(start_date, end_date) # Training data starts self.training_years years before start date and goes up to start date train = self.stock[(self.stock[\u0026#39;Date\u0026#39;] \u0026lt; start_date) \u0026amp; (self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (start_date - pd.DateOffset(years=self.training_years)))] # Testing data is specified in the range test = self.stock[(self.stock[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (self.stock[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] # Create and train the model model = self.create_model() model.fit(train) # Make a future dataframe and predictions future = model.make_future_dataframe(periods = 365, freq=\u0026#39;D\u0026#39;) future = model.predict(future) # Merge predictions with the known values test = pd.merge(test, future, on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) train = pd.merge(train, future, on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) # Calculate the differences between consecutive measurements test[\u0026#39;pred_diff\u0026#39;] = test[\u0026#39;yhat\u0026#39;].diff() test[\u0026#39;real_diff\u0026#39;] = test[\u0026#39;y\u0026#39;].diff() # Correct is when we predicted the correct direction test[\u0026#39;correct\u0026#39;] = (np.sign(test[\u0026#39;pred_diff\u0026#39;]) == np.sign(test[\u0026#39;real_diff\u0026#39;])) * 1 # Accuracy when we predict increase and decrease increase_accuracy = 100 * np.mean(test[test[\u0026#39;pred_diff\u0026#39;] \u0026gt; 0][\u0026#39;correct\u0026#39;]) decrease_accuracy = 100 * np.mean(test[test[\u0026#39;pred_diff\u0026#39;] \u0026lt; 0][\u0026#39;correct\u0026#39;]) # Calculate mean absolute error test_errors = abs(test[\u0026#39;y\u0026#39;] - test[\u0026#39;yhat\u0026#39;]) test_mean_error = np.mean(test_errors) train_errors = abs(train[\u0026#39;y\u0026#39;] - train[\u0026#39;yhat\u0026#39;]) train_mean_error = np.mean(train_errors) # Calculate percentage of time actual value within prediction range test[\u0026#39;in_range\u0026#39;] = False for i in test.index: if (test[\u0026#39;y\u0026#39;].iloc[i] \u0026lt; test[\u0026#39;yhat_upper\u0026#39;].iloc[i]) \u0026amp; (test[\u0026#39;y\u0026#39;].iloc[i] \u0026gt; test[\u0026#39;yhat_lower\u0026#39;].iloc[i]): test[\u0026#39;in_range\u0026#39;].iloc[i] = True in_range_accuracy = 100 * np.mean(test[\u0026#39;in_range\u0026#39;]) if not nshares: # Date range of predictions print(\u0026#39;\\nPrediction Range: {} to {}.\u0026#39;.format(start_date, end_date)) # Final prediction vs actual value print(\u0026#39;\\nPredicted price on {} = ${:.2f}.\u0026#39;.format(max(future[\u0026#39;ds\u0026#39;]), future[\u0026#39;yhat\u0026#39;].iloc[len(future) - 1])) print(\u0026#39;Actual price on {} = ${:.2f}.\\n\u0026#39;.format(max(test[\u0026#39;ds\u0026#39;]), test[\u0026#39;y\u0026#39;].iloc[len(test) - 1])) print(\u0026#39;Average Absolute Error on Training Data = ${:.2f}.\u0026#39;.format(train_mean_error)) print(\u0026#39;Average Absolute Error on Testing Data = ${:.2f}.\\n\u0026#39;.format(test_mean_error)) # Direction accuracy print(\u0026#39;When the model predicted an increase, the price increased {:.2f}% of the time.\u0026#39;.format(increase_accuracy)) print(\u0026#39;When the model predicted a decrease, the price decreased {:.2f}% of the time.\\n\u0026#39;.format(decrease_accuracy)) print(\u0026#39;The actual value was within the {:d}% confidence interval {:.2f}% of the time.\u0026#39;.format(int(100 * model.interval_width), in_range_accuracy)) # Reset the plot self.reset_plot() # Set up the plot fig, ax = plt.subplots(1, 1) # Plot the actual values ax.plot(train[\u0026#39;ds\u0026#39;], train[\u0026#39;y\u0026#39;], \u0026#39;ko-\u0026#39;, linewidth = 1.4, alpha = 0.8, ms = 1.8, label = \u0026#39;Observations\u0026#39;) ax.plot(test[\u0026#39;ds\u0026#39;], test[\u0026#39;y\u0026#39;], \u0026#39;ko-\u0026#39;, linewidth = 1.4, alpha = 0.8, ms = 1.8, label = \u0026#39;Observations\u0026#39;) # Plot the predicted values ax.plot(future[\u0026#39;ds\u0026#39;], future[\u0026#39;yhat\u0026#39;], \u0026#39;navy\u0026#39;, linewidth = 2.4, label = \u0026#39;Predicted\u0026#39;); # Plot the uncertainty interval as ribbon ax.fill_between(future[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), future[\u0026#39;yhat_upper\u0026#39;], future[\u0026#39;yhat_lower\u0026#39;], alpha = 0.6, facecolor = \u0026#39;gold\u0026#39;, edgecolor = \u0026#39;k\u0026#39;, linewidth = 1.4, label = \u0026#39;Confidence Interval\u0026#39;) # Put a vertical line at the start of predictions plt.vlines(x=min(test[\u0026#39;ds\u0026#39;]), ymin=min(future[\u0026#39;yhat_lower\u0026#39;]), ymax=max(future[\u0026#39;yhat_upper\u0026#39;]), colors = \u0026#39;r\u0026#39;, linestyles=\u0026#39;dashed\u0026#39;, label = \u0026#39;Prediction Start\u0026#39;) # Plot formatting plt.legend(loc = 2, prop={\u0026#39;size\u0026#39;: 8}); plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Price $\u0026#39;); plt.grid(linewidth=0.6, alpha = 0.6) plt.title(\u0026#39;{} Model Evaluation from {} to {}.\u0026#39;.format(self.symbol, start_date, end_date)); plt.show(); # If a number of shares is specified, play the game elif nshares: # Only playing the stocks when we predict the stock will increase test_pred_increase = test[test[\u0026#39;pred_diff\u0026#39;] \u0026gt; 0] test_pred_increase.reset_index(inplace=True) prediction_profit = [] # Iterate through all the predictions and calculate profit from playing for i, correct in enumerate(test_pred_increase[\u0026#39;correct\u0026#39;]): # If we predicted up and the price goes up, we gain the difference if correct == 1: prediction_profit.append(nshares * test_pred_increase[\u0026#39;real_diff\u0026#39;].iloc[i]) # If we predicted up and the price goes down, we lose the difference else: prediction_profit.append(nshares * test_pred_increase[\u0026#39;real_diff\u0026#39;].iloc[i]) test_pred_increase[\u0026#39;pred_profit\u0026#39;] = prediction_profit # Put the profit into the test dataframe test = pd.merge(test, test_pred_increase[[\u0026#39;ds\u0026#39;, \u0026#39;pred_profit\u0026#39;]], on = \u0026#39;ds\u0026#39;, how = \u0026#39;left\u0026#39;) test[\u0026#39;pred_profit\u0026#39;].iloc[0] = 0 # Profit for either method at all dates test[\u0026#39;pred_profit\u0026#39;] = test[\u0026#39;pred_profit\u0026#39;].cumsum().ffill() test[\u0026#39;hold_profit\u0026#39;] = nshares * (test[\u0026#39;y\u0026#39;] - float(test[\u0026#39;y\u0026#39;].iloc[0])) # Display information print(\u0026#39;You played the stock market in {} from {} to {} with {} shares.\\n\u0026#39;.format( self.symbol, start_date, end_date, nshares)) print(\u0026#39;When the model predicted an increase, the price increased {:.2f}% of the time.\u0026#39;.format(increase_accuracy)) print(\u0026#39;When the model predicted a decrease, the price decreased {:.2f}% of the time.\\n\u0026#39;.format(decrease_accuracy)) # Display some friendly information about the perils of playing the stock market print(\u0026#39;The total profit using the Prophet model = ${:.2f}.\u0026#39;.format(np.sum(prediction_profit))) print(\u0026#39;The Buy and Hold strategy profit = ${:.2f}.\u0026#39;.format(float(test[\u0026#39;hold_profit\u0026#39;].iloc[len(test) - 1]))) print(\u0026#39;\\nThanks for playing the stock market!\\n\u0026#39;) # Plot the predicted and actual profits over time self.reset_plot() # Final profit and final smart used for locating text final_profit = test[\u0026#39;pred_profit\u0026#39;].iloc[len(test) - 1] final_smart = test[\u0026#39;hold_profit\u0026#39;].iloc[len(test) - 1] # text location last_date = test[\u0026#39;ds\u0026#39;].iloc[len(test) - 1] text_location = (last_date - pd.DateOffset(months = 1)) plt.style.use(\u0026#39;dark_background\u0026#39;) # Plot smart profits plt.plot(test[\u0026#39;ds\u0026#39;], test[\u0026#39;hold_profit\u0026#39;], \u0026#39;b\u0026#39;, linewidth = 1.8, label = \u0026#39;Buy and Hold Strategy\u0026#39;) # Plot prediction profits plt.plot(test[\u0026#39;ds\u0026#39;], test[\u0026#39;pred_profit\u0026#39;], color = \u0026#39;g\u0026#39; if final_profit \u0026gt; 0 else \u0026#39;r\u0026#39;, linewidth = 1.8, label = \u0026#39;Prediction Strategy\u0026#39;) # Display final values on graph plt.text(x = text_location, y = final_profit + (final_profit / 40), s = \u0026#39;$%d\u0026#39; % final_profit, color = \u0026#39;g\u0026#39; if final_profit \u0026gt; 0 else \u0026#39;r\u0026#39;, size = 18) plt.text(x = text_location, y = final_smart + (final_smart / 40), s = \u0026#39;$%d\u0026#39; % final_smart, color = \u0026#39;g\u0026#39; if final_smart \u0026gt; 0 else \u0026#39;r\u0026#39;, size = 18); # Plot formatting plt.ylabel(\u0026#39;Profit (US $)\u0026#39;); plt.xlabel(\u0026#39;Date\u0026#39;); plt.title(\u0026#39;Predicted versus Buy and Hold Profits\u0026#39;); plt.legend(loc = 2, prop={\u0026#39;size\u0026#39;: 10}); plt.grid(alpha=0.2); plt.show() def retrieve_google_trends(self, search, date_range): # Set up the trend fetching object pytrends = TrendReq(hl=\u0026#39;en-US\u0026#39;, tz=360) kw_list = [search] try: # Create the search object pytrends.build_payload(kw_list, cat=0, timeframe=date_range[0], geo=\u0026#39;\u0026#39;, gprop=\u0026#39;news\u0026#39;) # Retrieve the interest over time trends = pytrends.interest_over_time() related_queries = pytrends.related_queries() except Exception as e: print(\u0026#39;\\nGoogle Search Trend retrieval failed.\u0026#39;) print(e) return return trends, related_queries def changepoint_date_analysis(self, search=None): self.reset_plot() model = self.create_model() # Use past self.training_years years of data train = self.stock[self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (self.max_date - pd.DateOffset(years = self.training_years))] model.fit(train) # Predictions of the training data (no future periods) future = model.make_future_dataframe(periods=0, freq=\u0026#39;D\u0026#39;) future = model.predict(future) train = pd.merge(train, future[[\u0026#39;ds\u0026#39;, \u0026#39;yhat\u0026#39;]], on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) changepoints = model.changepoints train = train.reset_index(drop=True) # Create dataframe of only changepoints change_indices = [] for changepoint in (changepoints): change_indices.append(train[train[\u0026#39;ds\u0026#39;] == changepoint].index[0]) c_data = train.iloc[change_indices, :] deltas = model.params[\u0026#39;delta\u0026#39;][0] c_data[\u0026#39;delta\u0026#39;] = deltas c_data[\u0026#39;abs_delta\u0026#39;] = abs(c_data[\u0026#39;delta\u0026#39;]) # Sort the values by maximum change c_data = c_data.sort_values(by=\u0026#39;abs_delta\u0026#39;, ascending=False) # Limit to 10 largest changepoints c_data = c_data[:10] # Separate into negative and positive changepoints cpos_data = c_data[c_data[\u0026#39;delta\u0026#39;] \u0026gt; 0] cneg_data = c_data[c_data[\u0026#39;delta\u0026#39;] \u0026lt; 0] # Changepoints and data if not search: print(\u0026#39;\\nChangepoints sorted by slope rate of change (2nd derivative):\\n\u0026#39;) print(c_data[[\u0026#39;Date\u0026#39;, \u0026#39;Adj. Close\u0026#39;, \u0026#39;delta\u0026#39;]][:5]) # Line plot showing actual values, estimated values, and changepoints self.reset_plot() # Set up line plot plt.plot(train[\u0026#39;ds\u0026#39;], train[\u0026#39;y\u0026#39;], \u0026#39;ko\u0026#39;, ms = 4, label = \u0026#39;Stock Price\u0026#39;) plt.plot(future[\u0026#39;ds\u0026#39;], future[\u0026#39;yhat\u0026#39;], color = \u0026#39;navy\u0026#39;, linewidth = 2.0, label = \u0026#39;Modeled\u0026#39;) # Changepoints as vertical lines plt.vlines(cpos_data[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), ymin = min(train[\u0026#39;y\u0026#39;]), ymax = max(train[\u0026#39;y\u0026#39;]), linestyles=\u0026#39;dashed\u0026#39;, color = \u0026#39;r\u0026#39;, linewidth= 1.2, label=\u0026#39;Negative Changepoints\u0026#39;) plt.vlines(cneg_data[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), ymin = min(train[\u0026#39;y\u0026#39;]), ymax = max(train[\u0026#39;y\u0026#39;]), linestyles=\u0026#39;dashed\u0026#39;, color = \u0026#39;darkgreen\u0026#39;, linewidth= 1.2, label=\u0026#39;Positive Changepoints\u0026#39;) plt.legend(prop={\u0026#39;size\u0026#39;:10}); plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Price ($)\u0026#39;); plt.title(\u0026#39;Stock Price with Changepoints\u0026#39;) plt.show() # Search for search term in google news # Show related queries, rising related queries # Graph changepoints, search frequency, stock price if search: date_range = [\u0026#39;%s %s\u0026#39; % (str(min(train[\u0026#39;Date\u0026#39;])), str(max(train[\u0026#39;Date\u0026#39;])))] # Get the Google Trends for specified terms and join to training dataframe trends, related_queries = self.retrieve_google_trends(search, date_range) if (trends is None) or (related_queries is None): print(\u0026#39;No search trends found for %s\u0026#39; % search) return print(\u0026#39;\\n Top Related Queries: \\n\u0026#39;) print(related_queries[search][\u0026#39;top\u0026#39;].head()) print(\u0026#39;\\n Rising Related Queries: \\n\u0026#39;) print(related_queries[search][\u0026#39;rising\u0026#39;].head()) # Upsample the data for joining with training data trends = trends.resample(\u0026#39;D\u0026#39;) trends = trends.reset_index(level=0) trends = trends.rename(columns={\u0026#39;date\u0026#39;: \u0026#39;ds\u0026#39;, search: \u0026#39;freq\u0026#39;}) # Interpolate the frequency trends[\u0026#39;freq\u0026#39;] = trends[\u0026#39;freq\u0026#39;].interpolate() # Merge with the training data train = pd.merge(train, trends, on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) # Normalize values train[\u0026#39;y_norm\u0026#39;] = train[\u0026#39;y\u0026#39;] / max(train[\u0026#39;y\u0026#39;]) train[\u0026#39;freq_norm\u0026#39;] = train[\u0026#39;freq\u0026#39;] / max(train[\u0026#39;freq\u0026#39;]) self.reset_plot() # Plot the normalized stock price and normalize search frequency plt.plot(train[\u0026#39;ds\u0026#39;], train[\u0026#39;y_norm\u0026#39;], \u0026#39;k-\u0026#39;, label = \u0026#39;Stock Price\u0026#39;) plt.plot(train[\u0026#39;ds\u0026#39;], train[\u0026#39;freq_norm\u0026#39;], color=\u0026#39;goldenrod\u0026#39;, label = \u0026#39;Search Frequency\u0026#39;) # Changepoints as vertical lines plt.vlines(cpos_data[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), ymin = 0, ymax = 1, linestyles=\u0026#39;dashed\u0026#39;, color = \u0026#39;r\u0026#39;, linewidth= 1.2, label=\u0026#39;Negative Changepoints\u0026#39;) plt.vlines(cneg_data[\u0026#39;ds\u0026#39;].dt.to_pydatetime(), ymin = 0, ymax = 1, linestyles=\u0026#39;dashed\u0026#39;, color = \u0026#39;darkgreen\u0026#39;, linewidth= 1.2, label=\u0026#39;Positive Changepoints\u0026#39;) # Plot formatting plt.legend(prop={\u0026#39;size\u0026#39;: 10}) plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Normalized Values\u0026#39;); plt.title(\u0026#39;%s Stock Price and Search Frequency for %s\u0026#39; % (self.symbol, search)) plt.show() # Predict the future price for a given range of days def predict_future(self, days=30): # Use past self.training_years years for training train = self.stock[self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (max(self.stock[\u0026#39;Date\u0026#39;] ) - pd.DateOffset(years=self.training_years))] model = self.create_model() model.fit(train) # Future dataframe with specified number of days to predict future = model.make_future_dataframe(periods=days, freq=\u0026#39;D\u0026#39;) future = model.predict(future) # Only concerned with future dates future = future[future[\u0026#39;ds\u0026#39;] \u0026gt;= max(self.stock[\u0026#39;Date\u0026#39;])] # Remove the weekends future = self.remove_weekends(future) # Calculate whether increase or not future[\u0026#39;diff\u0026#39;] = future[\u0026#39;yhat\u0026#39;].diff() future = future.dropna() # Find the prediction direction and create separate dataframes future[\u0026#39;direction\u0026#39;] = (future[\u0026#39;diff\u0026#39;] \u0026gt; 0) * 1 # Rename the columns for presentation future = future.rename(columns={\u0026#39;ds\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;yhat\u0026#39;: \u0026#39;estimate\u0026#39;, \u0026#39;diff\u0026#39;: \u0026#39;change\u0026#39;, \u0026#39;yhat_upper\u0026#39;: \u0026#39;upper\u0026#39;, \u0026#39;yhat_lower\u0026#39;: \u0026#39;lower\u0026#39;}) future_increase = future[future[\u0026#39;direction\u0026#39;] == 1] future_decrease = future[future[\u0026#39;direction\u0026#39;] == 0] # Print out the dates print(\u0026#39;\\nPredicted Increase: \\n\u0026#39;) print(future_increase[[\u0026#39;Date\u0026#39;, \u0026#39;estimate\u0026#39;, \u0026#39;change\u0026#39;, \u0026#39;upper\u0026#39;, \u0026#39;lower\u0026#39;]]) print(\u0026#39;\\nPredicted Decrease: \\n\u0026#39;) print(future_decrease[[\u0026#39;Date\u0026#39;, \u0026#39;estimate\u0026#39;, \u0026#39;change\u0026#39;, \u0026#39;upper\u0026#39;, \u0026#39;lower\u0026#39;]]) self.reset_plot() # Set up plot plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) matplotlib.rcParams[\u0026#39;axes.labelsize\u0026#39;] = 10 matplotlib.rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 8 matplotlib.rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 8 matplotlib.rcParams[\u0026#39;axes.titlesize\u0026#39;] = 12 # Plot the predictions and indicate if increase or decrease fig, ax = plt.subplots(1, 1, figsize=(8, 6)) # Plot the estimates ax.plot(future_increase[\u0026#39;Date\u0026#39;], future_increase[\u0026#39;estimate\u0026#39;], \u0026#39;g^\u0026#39;, ms = 12, label = \u0026#39;Pred. Increase\u0026#39;) ax.plot(future_decrease[\u0026#39;Date\u0026#39;], future_decrease[\u0026#39;estimate\u0026#39;], \u0026#39;rv\u0026#39;, ms = 12, label = \u0026#39;Pred. Decrease\u0026#39;) # Plot errorbars ax.errorbar(future[\u0026#39;Date\u0026#39;].dt.to_pydatetime(), future[\u0026#39;estimate\u0026#39;], yerr = future[\u0026#39;upper\u0026#39;] - future[\u0026#39;lower\u0026#39;], capthick=1.4, color = \u0026#39;k\u0026#39;,linewidth = 2, ecolor=\u0026#39;darkblue\u0026#39;, capsize = 4, elinewidth = 1, label = \u0026#39;Pred with Range\u0026#39;) # Plot formatting plt.legend(loc = 2, prop={\u0026#39;size\u0026#39;: 10}); plt.xticks(rotation = \u0026#39;45\u0026#39;) plt.ylabel(\u0026#39;Predicted Price (US $)\u0026#39;); plt.xlabel(\u0026#39;Date\u0026#39;); plt.title(\u0026#39;Predictions for %s\u0026#39; % self.symbol); plt.show() def changepoint_prior_validation(self, start_date=None, end_date=None,changepoint_priors = [0.001, 0.05, 0.1, 0.2]): # Default start date is two years before end of data # Default end date is one year before end of data if start_date is None: start_date = self.max_date - pd.DateOffset(years=2) if end_date is None: end_date = self.max_date - pd.DateOffset(years=1) # Convert to pandas datetime for indexing dataframe start_date = pd.to_datetime(start_date) end_date = pd.to_datetime(end_date) start_date, end_date = self.handle_dates(start_date, end_date) # Select self.training_years number of years train = self.stock[(self.stock[\u0026#39;Date\u0026#39;] \u0026gt; (start_date - pd.DateOffset(years=self.training_years))) \u0026amp; (self.stock[\u0026#39;Date\u0026#39;] \u0026lt; start_date)] # Testing data is specified by range test = self.stock[(self.stock[\u0026#39;Date\u0026#39;] \u0026gt;= start_date) \u0026amp; (self.stock[\u0026#39;Date\u0026#39;] \u0026lt;= end_date)] eval_days = (max(test[\u0026#39;Date\u0026#39;]) - min(test[\u0026#39;Date\u0026#39;])).days results = pd.DataFrame(0, index = list(range(len(changepoint_priors))), columns = [\u0026#39;cps\u0026#39;, \u0026#39;train_err\u0026#39;, \u0026#39;train_range\u0026#39;, \u0026#39;test_err\u0026#39;, \u0026#39;test_range\u0026#39;]) print(\u0026#39;\\nValidation Range {} to {}.\\n\u0026#39;.format(min(test[\u0026#39;Date\u0026#39;]), max(test[\u0026#39;Date\u0026#39;]))) # Iterate through all the changepoints and make models for i, prior in enumerate(changepoint_priors): results[\u0026#39;cps\u0026#39;].iloc[i] = prior # Select the changepoint self.changepoint_prior_scale = prior # Create and train a model with the specified cps model = self.create_model() model.fit(train) future = model.make_future_dataframe(periods=eval_days, freq=\u0026#39;D\u0026#39;) future = model.predict(future) # Training results and metrics train_results = pd.merge(train, future[[\u0026#39;ds\u0026#39;, \u0026#39;yhat\u0026#39;, \u0026#39;yhat_upper\u0026#39;, \u0026#39;yhat_lower\u0026#39;]], on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) avg_train_error = np.mean(abs(train_results[\u0026#39;y\u0026#39;] - train_results[\u0026#39;yhat\u0026#39;])) avg_train_uncertainty = np.mean(abs(train_results[\u0026#39;yhat_upper\u0026#39;] - train_results[\u0026#39;yhat_lower\u0026#39;])) results[\u0026#39;train_err\u0026#39;].iloc[i] = avg_train_error results[\u0026#39;train_range\u0026#39;].iloc[i] = avg_train_uncertainty # Testing results and metrics test_results = pd.merge(test, future[[\u0026#39;ds\u0026#39;, \u0026#39;yhat\u0026#39;, \u0026#39;yhat_upper\u0026#39;, \u0026#39;yhat_lower\u0026#39;]], on = \u0026#39;ds\u0026#39;, how = \u0026#39;inner\u0026#39;) avg_test_error = np.mean(abs(test_results[\u0026#39;y\u0026#39;] - test_results[\u0026#39;yhat\u0026#39;])) avg_test_uncertainty = np.mean(abs(test_results[\u0026#39;yhat_upper\u0026#39;] - test_results[\u0026#39;yhat_lower\u0026#39;])) results[\u0026#39;test_err\u0026#39;].iloc[i] = avg_test_error results[\u0026#39;test_range\u0026#39;].iloc[i] = avg_test_uncertainty print(results) # Plot of training and testing average errors self.reset_plot() plt.plot(results[\u0026#39;cps\u0026#39;], results[\u0026#39;train_err\u0026#39;], \u0026#39;bo-\u0026#39;, ms = 8, label = \u0026#39;Train Error\u0026#39;) plt.plot(results[\u0026#39;cps\u0026#39;], results[\u0026#39;test_err\u0026#39;], \u0026#39;r*-\u0026#39;, ms = 8, label = \u0026#39;Test Error\u0026#39;) plt.xlabel(\u0026#39;Changepoint Prior Scale\u0026#39;); plt.ylabel(\u0026#39;Avg. Absolute Error ($)\u0026#39;); plt.title(\u0026#39;Training and Testing Curves as Function of CPS\u0026#39;) plt.grid(color=\u0026#39;k\u0026#39;, alpha=0.3) plt.xticks(results[\u0026#39;cps\u0026#39;], results[\u0026#39;cps\u0026#39;]) plt.legend(prop={\u0026#39;size\u0026#39;:10}) plt.show(); # Plot of training and testing average uncertainty self.reset_plot() plt.plot(results[\u0026#39;cps\u0026#39;], results[\u0026#39;train_range\u0026#39;], \u0026#39;bo-\u0026#39;, ms = 8, label = \u0026#39;Train Range\u0026#39;) plt.plot(results[\u0026#39;cps\u0026#39;], results[\u0026#39;test_range\u0026#39;], \u0026#39;r*-\u0026#39;, ms = 8, label = \u0026#39;Test Range\u0026#39;) plt.xlabel(\u0026#39;Changepoint Prior Scale\u0026#39;); plt.ylabel(\u0026#39;Avg. Uncertainty ($)\u0026#39;); plt.title(\u0026#39;Uncertainty in Estimate as Function of CPS\u0026#39;) plt.grid(color=\u0026#39;k\u0026#39;, alpha=0.3) plt.xticks(results[\u0026#39;cps\u0026#39;], results[\u0026#39;cps\u0026#39;]) plt.legend(prop={\u0026#39;size\u0026#39;:10}) plt.show(); import pandas as pd # 讀入資料 df = pd.read_csv(\u0026#39;/content/DEXVZUS_out.csv\u0026#39;, index_col=\u0026#39;ds\u0026#39;, parse_dates=[\u0026#39;ds\u0026#39;]) # 先用中位數補齊 NaN（每欄各自補自己的中位數） df_filled = df.fillna(df.median(numeric_only=True)) # 再執行 squeeze（若只有一欄，才會變成 Series） price = df_filled.squeeze() # 檢查結果 print(price.tail()) DEXVZUS= Stocker(price) model, model_data = DEXVZUS.create_prophet_model(days=1 ) import numpy as np import pandas as pd from sklearn.preprocessing import MinMaxScaler from tensorflow.keras.models import Sequential from tensorflow.keras.layers import LSTM, Dense, Dropout import matplotlib.pyplot as plt import matplotlib.dates as mdates # Prepare data df = pd.read_csv(\u0026#39;output.csv\u0026#39;) # Replace with your actual data loading method df[\u0026#39;DATE\u0026#39;] = pd.to_datetime(df[\u0026#39;DATE\u0026#39;]) df = df.rename(columns={\u0026#39;DATE\u0026#39;: \u0026#39;date\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;value\u0026#39;}) # Create lag features (滯後特徵) def create_lag_features(data, lag=1): lagged_data = data.copy() for i in range(1, lag+1): lagged_data[f\u0026#39;lag_{i}\u0026#39;] = lagged_data[\u0026#39;value\u0026#39;].shift(i) return lagged_data.dropna() # Create lag features (加入前幾天的價格) lag = 3 # Use last 3 days to predict df_lagged = create_lag_features(df, lag) # Scale the data scaler = MinMaxScaler() values_scaled = scaler.fit_transform(df_lagged[\u0026#39;value\u0026#39;].values.reshape(-1, 1)) # Create sequences for LSTM def create_sequences(data, seq_length): X, y = [], [] for i in range(len(data) - seq_length): X.append(data[i:(i + seq_length)]) y.append(data[i + seq_length]) return np.array(X), np.array(y) # Create sequences seq_length = 12 # Using 12 months of data to predict next month X, y = create_sequences(values_scaled, seq_length) # Split data into train and test sets train_size = int(len(X) * 0.8) X_train, X_test = X[:train_size], X[train_size:] y_train, y_test = y[:train_size], y[train_size:] # Create the LSTM model with modified architecture model = Sequential([ LSTM(50, return_sequences=True, input_shape=(seq_length, 1)), Dropout(0.3), # Increased Dropout to avoid overfitting LSTM(50, return_sequences=False), Dropout(0.3), Dense(25, activation=\u0026#39;relu\u0026#39;), Dense(1, activation=\u0026#39;relu\u0026#39;) # 使用 relu 確保輸出為正數 ]) # Compile the model with different optimizer settings model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mse\u0026#39;) # Train the model with more epochs and validation split history = model.fit( X_train, y_train, epochs=50, # Increased epochs for better training batch_size=32, validation_split=0.1, verbose=1 ) # Predict for 2025/1/8 future_date = pd.Timestamp(\u0026#39;2025-07-27\u0026#39;) last_sequence = values_scaled[-seq_length:] last_sequence = last_sequence.reshape(1, seq_length, 1) future_pred_scaled = model.predict(last_sequence) future_pred = scaler.inverse_transform(future_pred_scaled) print(f\u0026#39;Predicted value for 2025/1/8: ${future_pred[0][0]:,.2f}\u0026#39;) # Plot with enhanced date formatting plt.figure(figsize=(12, 6)) plt.plot(df[\u0026#39;date\u0026#39;], df[\u0026#39;value\u0026#39;], label=\u0026#39;Actual Values\u0026#39;, marker=\u0026#39;o\u0026#39;) plt.plot(future_date, future_pred[0][0], \u0026#39;r*\u0026#39;, markersize=15, label=\u0026#39;Predicted Value\u0026#39;) # Format x-axis to show dates in YYYY/MM/DD format plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y/%m/%d\u0026#39;)) plt.gcf().autofmt_xdate() # Rotate and align the tick labels plt.title(\u0026#39;Time Series Prediction\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Value ($)\u0026#39;) plt.legend() plt.grid(True) # Format y-axis to show dollar values plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f\u0026#39;${x:,.2f}\u0026#39;)) print(f\u0026#39;Predicted value for 2025/1/8: ${future_pred[0][0]:,.2f}\u0026#39;) print(f\u0026#39;Actual value: $369,980.57\u0026#39;) print(f\u0026#39;Absolute error: ${abs(future_pred[0][0] - 369980.57):,.2f}\u0026#39;) print(f\u0026#39;Relative error: {abs(future_pred[0][0] - 369980.57)/369980.57*100:.2f}%\u0026#39;) plt.tight_layout() plt.show() ","permalink":"https://s0914712.github.io/blog/post-9/","tags":["AI","Military","PLA","軍事動態","軍演","深度學習"],"title":"只靠每天的架次數預測的Prophet與LSTM 模型"},{"categories":["Man Over Board Model"],"contents":" 1. 概述 (What \u0026amp; Why) NetCDF 是一套為「科學陣列型資料」設計的 自描述 (self-describing)、跨平台 (portable)、可擴充 (extensible) 的資料模型與檔案格式，同時也包含一組 API 函式庫。典型應用領域包含：氣象、海洋、氣候、遙測、模式模擬（如大氣模式、海流、浪場）等。\n2. 核心資料模型 (Classic Data Model) 元素 意義 特點 Dimension（維度） 定義陣列的軸長度與名稱，例如 time, lat, lon, depth 可標記一個為 unlimited（無界/可增長），常用於時間序列 Variable（變數） 具名的 N 維陣列，使用一組已定義維度，並附屬屬性 儲存實際科學數值，如 temperature(time, depth, lat, lon) Attribute（屬性） 附掛在檔案 (global) 或變數上的「鍵值對」型 metadata 描述單位、長描述、縮放係數、缺值等 Data（資料） 各變數的陣列值本體 支援多種基本型別 (byte, char, short, int, float, double, int64, etc.) 3. NetCDF Header 範例與逐行解釋 原始 Header netcdf example { dimensions: time = UNLIMITED ; // (10 currently) lat = 181 ; lon = 360 ; variables: double time(time) ; time:units = \u0026#34;hours since 2025-07-20 00:00:00\u0026#34; ; time:calendar = \u0026#34;gregorian\u0026#34; ; float lat(lat) ; lat:units = \u0026#34;degrees_north\u0026#34; ; float lon(lon) ; lon:units = \u0026#34;degrees_east\u0026#34; ; float temperature(time, lat, lon) ; temperature:long_name = \u0026#34;Sea surface temperature\u0026#34; ; temperature:units = \u0026#34;K\u0026#34; ; temperature:_FillValue = -9999.f ; temperature:scale_factor = 0.01f ; temperature:add_offset = 273.15f ; // global attributes: :title = \u0026#34;Demo SST\u0026#34; ; :history = \u0026#34;2025-07-20T12:00:00Z created\u0026#34; ; } 逐段說明 維度宣告 dimensions: time = UNLIMITED ; // (10 currently) lat = 181 ; lon = 360 ; 宣告 3 個維度：time、lat、lon。 time = UNLIMITED 表示此維度可增長（appendable），目前已有 10 筆（索引 0..9）。 lat = 181、lon = 360 為固定長度。例如：lat 可能為 -90 到 90（含端點，共 181 個點，步距 1°）；lon 可能為 0 到 359（共 360 個點）。 時間變數 double time(time) ; time:units = \u0026#34;hours since 2025-07-20 00:00:00\u0026#34; ; time:calendar = \u0026#34;gregorian\u0026#34; ; time(time)：1 維時間座標變數。 units 採 CF 規範：「\u0026lt;時間單位\u0026gt; since \u0026lt;參考時間\u0026gt;」。此處表示「自 2025-07-20 00:00:00 起算的小時數」。 例：time[0] = 0 → 2025-07-20 00:00 time[1] = 6 → 2025-07-20 06:00 calendar=\u0026quot;gregorian\u0026quot; 指定日曆系統。 經緯度座標變數 float lat(lat) ; lat:units = \u0026#34;degrees_north\u0026#34; ; float lon(lon) ; lon:units = \u0026#34;degrees_east\u0026#34; ; 座標變數（coordinate variables）：名稱與維度相同。 提供每個格點的地理位置（緯度、經度）。 資料變數 float temperature(time, lat, lon) ; temperature:long_name = \u0026#34;Sea surface temperature\u0026#34; ; temperature:units = \u0026#34;K\u0026#34; ; temperature:_FillValue = -9999.f ; temperature:scale_factor = 0.01f ; temperature:add_offset = 273.15f ; temperature 為 3 維資料：time × lat × lon。 使用「打包（packed）」策略：以 scale_factor 與 add_offset 壓縮存放範圍。 解碼：physical_value = scale_factor * stored_value + add_offset。 例：stored = 2000 → 0.01 * 2000 + 273.15 = 293.15 K。 _FillValue = -9999.f：缺值標記；讀取後通常轉換為 NaN。 long_name 與 units 為語意與顯示用 metadata。 全域屬性 :title = \u0026#34;Demo SST\u0026#34; ; :history = \u0026#34;2025-07-20T12:00:00Z created\u0026#34; ; title：檔案標題。 history：處理沿革；每次加工可附加一段（建議包含時間戳與動作）。 4. 對應速查 區塊 重點 補充 dimensions 定義各軸大小，UNLIMITED 允許追加 一檔可有 0 或多個 unlimited（建議僅 1 個） time 變數 時間軸與日曆 使用 CF 規範「units + calendar」 lat/lon 空間座標 可搭配 standard_name=\u0026quot;latitude\u0026quot; 等 temperature 主資料陣列 打包機制節省空間 global attributes 整體描述與追溯 history 建議 ISO 8601 5. Packed value 解碼範例（Python） physical = scale_factor * stored + add_offset physical[stored == fill_value] = np.nan 需要再擴充成 CF Conventions 教學、或輸出成 PDF / Word 版本，可再告訴我。\n6. 使用 xarray 讀取與操作 NetCDF 以下示範使用 xarray（結合 netCDF4 或 h5netcdf 後端）讀取、檢視、分析與寫回 NetCDF 的典型流程。\n6.1 安裝 pip install xarray netCDF4 cftime dask[complete] # 若需要壓縮或平行最佳化，可再安裝 zarr, h5netcdf 等 6.2 基本開檔 import xarray as xr ds = xr.open_dataset(\u0026#39;example.nc\u0026#39;) # 延遲讀取 (lazy)；僅讀 header/metadata print(ds) # 類似 ncdump -h + 變數概觀 open_dataset 預設是 lazy：真正的資料直到運算或 .load() 才讀入記憶體。\n6.3 取出變數與座標 t = ds[\u0026#39;temperature\u0026#39;] # DataArray lat = ds[\u0026#39;lat\u0026#39;] lon = ds[\u0026#39;lon\u0026#39;] print(t.dims, t.shape) # (\u0026#39;time\u0026#39;,\u0026#39;lat\u0026#39;,\u0026#39;lon\u0026#39;), (10,181,360) 6.4 自動解碼時間、scale_factor 與缺值 xarray 預設會依 CF Conventions 自動：\n將 time 轉為 datetime64 或 cftime 物件。 套用 scale_factor / add_offset。 將 _FillValue/missing_value 轉為 NaN。 可停用：xr.open_dataset('example.nc', decode_times=False, mask_and_scale=False)。\n6.5 選取與切片 # 依座標值切片（最近鄰或對齊） subset = t.sel(time=\u0026#39;2025-07-20T06:00:00\u0026#39;, lat=slice(-10,10), lon=slice(120,150)) # 依索引切片 t0 = t.isel(time=0) 6.6 計算（懶加速） # 平均一段時間與空間（仍 lazy） mean_equatorial = t.sel(lat=slice(-5,5)).mean(dim=(\u0026#39;time\u0026#39;,\u0026#39;lat\u0026#39;)) # 觸發實際計算 result = mean_equatorial.compute() # 或 .load() 若安裝與設定 Dask，xarray 會自動用 Dask array，便於大檔案平行/分塊運算。\n6.7 設定 Chunk（適合大檔案） ds = xr.open_dataset(\u0026#39;example.nc\u0026#39;, chunks={\u0026#39;time\u0026#39;: 50}) chunks 對應 Dask 分塊大小；與 NetCDF chunk 不同，但影響執行時分佈計算。 建議 time 維度 chunk 為常用分析窗口倍數。 6.8 多檔拼接 (Concatenation) 與合併 # 多檔沿 time 座標自動串接 ds_all = xr.open_mfdataset(\u0026#39;var_2025_0*.nc\u0026#39;, combine=\u0026#39;by_coords\u0026#39;, parallel=True, data_vars=\u0026#39;minimal\u0026#39;, coords=\u0026#39;minimal\u0026#39;, compat=\u0026#39;override\u0026#39;) 常見參數：\n參數 作用 combine='by_coords' 依座標值（如 time）對齊拼接 parallel=True 啟用 Dask 平行 data_vars='minimal' 避免重複合併變數屬性衝突 compat='override' 屬性衝突時採第一份 6.9 去除重複時間與排序 if \u0026#39;time\u0026#39; in ds_all.coords: ds_all = ds_all.sortby(\u0026#39;time\u0026#39;) _, idx = xr.unique(ds_all.time, return_index=True) if len(idx) != ds_all.sizes[\u0026#39;time\u0026#39;]: ds_all = ds_all.isel(time=idx.sortby(idx)) 6.10 加入新計算結果 ds_all[\u0026#39;temp_anomaly\u0026#39;] = ds_all.temperature - ds_all.temperature.mean(\u0026#39;time\u0026#39;) 新增的 DataArray 會自動繼承對齊座標。\n6.11 寫回 NetCDF encoding = { \u0026#39;temperature\u0026#39;: { \u0026#39;zlib\u0026#39;: True, \u0026#39;complevel\u0026#39;: 4, \u0026#39;shuffle\u0026#39;: True, # 指定 chunksizes (影響最終 NetCDF 物理 chunk) \u0026#39;chunksizes\u0026#39;: (50, ds_all.dims[\u0026#39;lat\u0026#39;], ds_all.dims[\u0026#39;lon\u0026#39;]) }, \u0026#39;temp_anomaly\u0026#39;: {\u0026#39;zlib\u0026#39;: True, \u0026#39;complevel\u0026#39;: 4, \u0026#39;shuffle\u0026#39;: True} } ds_all.to_netcdf(\u0026#39;merged_with_anomaly.nc\u0026#39;, encoding=encoding) 注意：\nchunksizes 是寫入 NetCDF 物理 chunk（與 open_dataset(..., chunks=...) 的記憶體分塊不同）。 設定壓縮層級 1–6 通常平衡速度與壓縮比。 6.12 與 Pandas 互動 # 取某經緯點時間序列，轉為 Pandas Series pt_series = ds_all.temperature.sel(lat=0.0, lon=140.0, method=\u0026#39;nearest\u0026#39;) ser = pt_series.to_pandas() 6.13 常見錯誤排查 情況 原因 解法 ValueError: conflicting sizes for dimension 'time' 多檔 time 維度無法對齊 改用 combine='nested' + 手動指定 concat_dim 或檢查時間座標值 時間無法解析 (NaT) calendar 不支援 datetime64 使用 xr.open_dataset(..., use_cftime=True) 記憶體不足 .load() 讀入太大 保持 lazy，使用 .compute() 只在需要時觸發；調整 chunks 運算很慢 chunk 不佳或過多小檔 重新設定 chunk；先用 NCO/CDO 合併減少檔案數 6.14 快速範例（從讀到分析再輸出） import xarray as xr # 1. 讀多檔 files = \u0026#39;var_2025_0*.nc\u0026#39; ds = xr.open_mfdataset(files, combine=\u0026#39;by_coords\u0026#39;, chunks={\u0026#39;time\u0026#39;: 48}) # 2. 時間排序 ds = ds.sortby(\u0026#39;time\u0026#39;) # 3. 計算季平均 (假設 time 是 hourly 資料) season_mean = ds.temperature.resample(time=\u0026#39;3M\u0026#39;).mean() # 4. 儲存結果 season_mean.to_netcdf(\u0026#39;temperature_3M_mean.nc\u0026#39;) 6.15 小抄 (Cheat Sheet) 任務 做法 備註 開檔 xr.open_dataset() lazy, metadata only 多檔 xr.open_mfdataset() 自動拼接座標 選取 .sel() / .isel() 值 vs 索引 聚合 .mean() .sum() .resample() 支援多維 新變數 ds['new']=... 座標自動對齊 寫檔 .to_netcdf(encoding=...) 控制壓縮/chunk 去重 xr.unique + isel 需排序先 時間重抽樣 .resample(time='D').mean() 需時間為 datetime 類型 轉 Pandas .to_pandas() 只適用 1D 座標 若需要：我可以再加上 Zarr 雲端存取流程、平行 Dask 設定 或 CF 規範標準名稱補充，告訴我即可。\n","permalink":"https://s0914712.github.io/blog/post-8/","tags":["人員落水","軌跡預測","漂流"],"title":"從零開始建立人員落水軌跡預測模型(一)NETCDF檔案介紹"},{"categories":["History and Taiwan"],"contents":" 歷史上的「蟒蛇」策略 「蟒蛇」一詞的起源 海軍司令唐華日前接受《經濟學人》訪問時提到，中共正企圖利用「蟒蛇策略」（Anaconda Strategy）壓縮臺灣，並隨時準備執行封鎖。「蟒蛇」一詞可追溯至美國內戰（1861–1865）期間，插畫家 J. B. 艾略特創作了一幅名為《史考特的大蟒蛇計畫》（Scott\u0026rsquo;s Great Snake）的插畫，諷刺北方聯邦指揮官溫菲爾德·史考特（Winfield Scott）提出的戰爭計畫。\n史考特的計畫主張通過控制密西西比河並封鎖南方邦聯海岸線來削弱南方的經濟，實現戰爭的最終勝利。儘管一開始受到了大量批評，但隨著戰爭進展，這一策略逐漸被採用，並對南方的經濟造成致命影響，最終促成了南方的投降。\n歷史中的「蟒蛇」策略案例 「蟒蛇」策略早於美國內戰便已出現。以下是歷史上一些著名的封鎖戰略案例：\n1. 英荷戰爭（1652–1674） 背景：英國封鎖荷蘭的海上貿易路線。 結果：削弱荷蘭經濟與海軍實力，英國最終勝利。 2. 拿破崙戰爭（1803–1815） 背景：英國對法國進行海上封鎖，阻止港口補給；拿破崙則以「大陸政策」反制。 結果：俄國退出「大陸政策」，導致拿破崙東征失敗。 3. 第一次世界大戰（1914–1918） 背景：英國海軍封鎖德國補給線，導致德國物資短缺。 結果：德國實施無限制潛艇戰，誘發美國參戰，最終德國戰敗。 4. 第二次世界大戰（1939–1945） 美國對日石油禁運：1941 年美國對日本實施石油禁運，對日本經濟造成壓力，並間接引發珍珠港事件。 德國的狼群戰術：U 型潛艇封鎖盟軍補給線，初期戰果顯著，但隨雷達與護航技術改進最終失敗。 5. 福克蘭戰爭（1982） 背景：英國在福克蘭群島周圍建立海上封鎖區。 結果：成功切斷阿根廷的補給與增援，助英國獲得最終勝利。 現代「蟒蛇」策略與臺灣議題 《經濟學人》指出，中共可能利用「隔離」（Quarantine）與「封鎖」（Blockade）策略施壓臺灣：\n隔離：由海警主導，限制臺灣貿易進出口，造成經濟壓力，但未完全中斷。 封鎖：由軍方主導，幾乎完全斷絕物資流入，迫使臺灣接受條件。 如同歷史中的案例，成功的封鎖可能帶來戰略優勢，但敵方若能掙脫封鎖，封鎖國將失去優勢。\n結語 歷史表明，「蟒蛇」策略雖具威脅，但並非必勝之法。無論是封鎖還是反制，都需要考慮國際動態、技術創新與軍事實力的綜合作用。針對中共的可能行動，臺灣與盟國應做好多層次準備，以避免落入「蟒蛇」的束縛。\n參考資料 Elliott, J. B. \u0026ldquo;The Great Snake.\u0026rdquo; Political cartoon, 1861. Library of Congress Geography and Map Division. Jones, Bruce D. 海權經濟大未來（The Big Future of Maritime Power and Economy），台北：遠流出版社，2022。 Lin, Bonny, Brian Hart, Matthew P. Funaiole, Samantha Lu, and Truly Tinsley. \u0026ldquo;How China Could Quarantine Taiwan: Mapping Out Two Possible Scenarios.\u0026rdquo; Center for Strategic and International Studies (CSIS), June 5, 2024.\n閱讀全文 \u0026ldquo;An Interview with Admiral Tang Hua: China Is Using an \u0026lsquo;Anaconda Strategy\u0026rsquo; to Squeeze Taiwan.\u0026rdquo; The Economist, October 3, 2024.\n閱讀全文 Krasnicki, Thomas A. \u0026ldquo;Considering the Utility of Modern Blockade in a Protracted Conflict with China.\u0026rdquo; Joint Force Quarterly 115 (4th Quarter 2024): 44-46. Donald Kagan. On the Origins of War and the Preservation of Peace. New York: Anchor Books, 1996. ","permalink":"https://s0914712.github.io/blog/post-7/","tags":["Anaconda plan","history","Bloackade","Taiwan"],"title":"歷史上的「蟒蛇」策略"},{"categories":["AI解決軍事問題"],"contents":" 中共的航母每次出航，海軍的弟兄又得要出去跟監，壓力超大，有沒有辦法預知他的出現呢? 我們蒐集到日本防衛省統合幕僚監部 下載 2023-2024年經過日本的航母動態 整理成csv檔 資料集長得像下面這樣\nDATE Intel BZK battleshi1(\u0026lt;3) battleshi1(\u0026gt;3) carrier WZ7 R_Navy H6 Y-9 Russia Air Warning Taiwan Air Activity Taiwan 1LA Exerise month in 5 Eay inetel in 5 Eay Russiashi1 in 5 Eay battleshi1 in5EayH6Y9 is5datCARRIER 20230101 K K K 1 19 0 1 0 0 1 0 FALSE 20230102 K K K 1 0 0 1 0 0 2 0 TRUE 20230103 1 0 0 1 0 0 4 2 TRUE 20230104 T T 1 3 0 1 0 0 1 0 FALSE 20230105 1 3 0 1 1 0 3 0 FALSE 20230106 1 3 0 1 1 0 3 0 FALSE 試著把各種不同的機種出現都列出來 看機器學習 能不能辦到預測航母的出現 應該用LSTM 來做會比較準確 但實驗性質 我們就把5年內出現與否當作一個參數 使用RrandomForest 或XGBoost 分類法就好 點我下載 資料集說明\ncarrier 是我們想要預測的對象 Intel 情報船 BZK 無人機 battleship 戰艦 R_Navy 俄羅斯海軍 Russia Air俄羅斯空軍 7, Warning 航行警告 發布 Taiwan Air Activity 臺灣地區軍事動態 in 5 Day \u0026hellip; 在五天內是否有出現\u0026hellip;. 開始讀取資料 寫程式 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report data_path=\u0026#39;Japan.csv\u0026#39; def load_and_preprocess_data(data_path): \u0026#34;\u0026#34;\u0026#34; Load and preprocess the data \u0026#34;\u0026#34;\u0026#34; df = pd.read_csv(data_path) # Check for unexpected values in carrier column expected_values = {\u0026#39;K\u0026#39;, \u0026#39;N\u0026#39;, \u0026#39;E\u0026#39;, np.nan} unexpected_values = set(df[\u0026#39;carrier\u0026#39;].unique()) - expected_values if unexpected_values: print(\u0026#34;\\nWarning: Unexpected values found in carrier column:\u0026#34;) print(f\u0026#34;Unexpected values: {unexpected_values}\u0026#34;) print(\u0026#34;\\nRows with unexpected values:\u0026#34;) for value in unexpected_values: unexpected_rows = df[df[\u0026#39;carrier\u0026#39;] == value] print(f\u0026#34;\\nValue \u0026#39;{value}\u0026#39; appears in rows:\u0026#34;) print(unexpected_rows[[\u0026#39;DATE\u0026#39;, \u0026#39;carrier\u0026#39;]].to_string()) df[\u0026#39;carrier\u0026#39;] = df[\u0026#39;carrier\u0026#39;].fillna(\u0026#39;N\u0026#39;) return df def prepare_features(df): \u0026#34;\u0026#34;\u0026#34; Prepare features for the model \u0026#34;\u0026#34;\u0026#34; features = [ \u0026#39;Intel\u0026#39;, \u0026#39;BZK\u0026#39;, \u0026#39;battleship(\u0026lt;3)\u0026#39;, \u0026#39;battleship(\u0026gt;3)\u0026#39;, \u0026#39;WZ7\u0026#39;, \u0026#39;R_Navy\u0026#39;, \u0026#39;H6\u0026#39;, \u0026#39;Y-9\u0026#39;, \u0026#39;Russia Air\u0026#39;, \u0026#39;Warning\u0026#39;, \u0026#39;Taiwan Air Activity\u0026#39;, \u0026#39;Taiwan PLA Exerise\u0026#39;, \u0026#39;month\u0026#39;, \u0026#39;in 5 day intel\u0026#39;, \u0026#39;in 5 day Russiaship\u0026#39;, \u0026#39;in 5 day battleship\u0026#39;, \u0026#39;in5dayH6Y\u0026#39; ] # Verify available features available_features = [f for f in features if f in df.columns] # Data preprocessing for feature in available_features: df[feature] = pd.to_numeric(df[feature], errors=\u0026#39;coerce\u0026#39;).fillna(0) return df[available_features], available_features def train_model(X, y): \u0026#34;\u0026#34;\u0026#34; Train the Random Forest model with handling for small datasets \u0026#34;\u0026#34;\u0026#34; le = LabelEncoder() y_encoded = le.fit_transform(y) # Check class distribution unique_classes, class_counts = np.unique(y_encoded, return_counts=True) min_samples = min(class_counts) if min_samples \u0026lt; 2: print(f\u0026#34;Warning: Very small dataset detected. Using all data for training.\u0026#34;) model = RandomForestClassifier( n_estimators=200, max_depth=5, min_samples_split=2, min_samples_leaf=1, random_state=42 ) model.fit(X, y_encoded) return model, le, (X, y_encoded) else: # Normal split and training if enough samples X_train, X_test, y_train, y_test = train_test_split( X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded ) model = RandomForestClassifier( n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42 ) model.fit(X_train, y_train) return model, le, (X_test, y_test) def evaluate_model(model, X_test, y_test): \u0026#34;\u0026#34;\u0026#34; Evaluate model performance with proper handling of all classes \u0026#34;\u0026#34;\u0026#34; y_pred = model.predict(X_test) print(\u0026#34;\\nModel Evaluation:\u0026#34;) print(f\u0026#34;Accuracy: {accuracy_score(y_test, y_pred):.4f}\u0026#34;) # Get actual unique classes from the data unique_classes = sorted(np.unique(y_test)) # Get class names from label encoder class_names = [\u0026#39;K\u0026#39;, \u0026#39;N\u0026#39;, \u0026#39;E\u0026#39;] # Ensure we have all class names for the report actual_class_names = [class_names[i] for i in unique_classes] print(\u0026#34;\\nClassification Report:\u0026#34;) try: print(classification_report(y_test, y_pred, target_names=actual_class_names)) except Exception as e: print(\u0026#34;Detailed class-wise metrics:\u0026#34;) # Manual calculation of metrics for each class for class_idx, class_name in zip(unique_classes, actual_class_names): class_mask = (y_test == class_idx) class_pred_mask = (y_pred == class_idx) class_correct = np.sum((y_test == y_pred) \u0026amp; class_mask) class_total = np.sum(class_mask) class_precision = np.sum((y_test == y_pred) \u0026amp; class_pred_mask) / (np.sum(class_pred_mask) + 1e-10) class_recall = class_correct / (class_total + 1e-10) class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall + 1e-10) print(f\u0026#34;\\nClass: {class_name}\u0026#34;) print(f\u0026#34;Samples: {class_total}\u0026#34;) print(f\u0026#34;Precision: {class_precision:.4f}\u0026#34;) print(f\u0026#34;Recall: {class_recall:.4f}\u0026#34;) print(f\u0026#34;F1-score: {class_f1:.4f}\u0026#34;) def predict_carrier(model, le, features, new_data): \u0026#34;\u0026#34;\u0026#34; Make prediction for new data \u0026#34;\u0026#34;\u0026#34; df_new = pd.DataFrame([new_data]) for feature in features: if feature not in df_new.columns: df_new[feature] = 0 df_new = df_new[features] pred = model.predict(df_new) prob = model.predict_proba(df_new) return le.inverse_transform(pred)[0], prob[0] if __name__ == \u0026#34;__main__\u0026#34;: data_path = \u0026#34;Japan.csv\u0026#34; try: print(\u0026#34;Loading data from:\u0026#34;, data_path) # Load and preprocess data df = load_and_preprocess_data(data_path) # Prepare features X, features = prepare_features(df) y = df[\u0026#39;carrier\u0026#39;] # Print basic statistics total = len(df) value_counts = y.value_counts() print(\u0026#34;\\nDetailed carrier value counts:\u0026#34;) print(value_counts) print(\u0026#34;\\nBasic Statistics:\u0026#34;) print(f\u0026#34;Total Records: {total}\u0026#34;) # Print counts for each value, including unexpected ones for value in value_counts.index: count = value_counts[value] percentage = count/total print(f\u0026#34;Carrier appearances as \u0026#39;{value}\u0026#39;: {count} ({percentage:.2%})\u0026#34;) # Train model model, label_encoder, (X_test, y_test) = train_model(X, y) # Evaluate model evaluate_model(model, X_test, y_test) # Feature importance importance = pd.DataFrame({ \u0026#39;feature\u0026#39;: features, \u0026#39;importance\u0026#39;: model.feature_importances_ }).sort_values(\u0026#39;importance\u0026#39;, ascending=False) print(\u0026#34;\\nFeature Importance:\u0026#34;) print(importance) # Example prediction(這邊給一個樣本讓他預測) new_data = {f: 0 for f in features} new_data.update({ \u0026#39;R_Navy\u0026#39;: 1, \u0026#39;month\u0026#39;: 1, \u0026#39;in 5 day Russiaship\u0026#39;: 1 }) prediction, probabilities = predict_carrier(model, label_encoder, features, new_data) print(\u0026#34;\\nPrediction Results:\u0026#34;) print(f\u0026#34;Predicted Location: {prediction}\u0026#34;) # Print probabilities for all classes class_names = label_encoder.classes_ # Use actual classes from encoder for class_name, prob in zip(class_names, probabilities): print(f\u0026#34;Probability of {class_name}: {prob:.2%}\u0026#34;) except FileNotFoundError: print(f\u0026#34;Error: File \u0026#39;{data_path}\u0026#39; not found\u0026#34;) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) 以下是程式的結果 Loading data from: Japan.csv Detailed carrier value counts: carrier N 617 K 113 Name: count, dtype: int64 Basic Statistics: Total Records: 730 Carrier appearances as \u0026#39;N\u0026#39;: 617 (84.52%) 沒出現航母的樣本數 Carrier appearances as \u0026#39;K\u0026#39;: 113 (15.48%) 出現航母的樣本數 模型的評估 Model Evaluation: Accuracy: 0.9863 Classification Report: precision recall f1-score support 預測航母出現的準確率 K 0.96 0.96 0.96 23 預測航母沒出現的準確率 N 0.99 0.99 0.99 123 accuracy 0.99 146 macro avg 0.97 0.97 0.97 146 weighted avg 0.99 0.99 0.99 146 #recall 為預測為正的數量(有可能沒出現也給他預測出現) Feature Importance: feature importance 10 is5datCARRIER 0.893500 9 month 0.059949 8 Taiwan Air Activity 0.036262 7 Warning 0.010289 0 Intel 0.000000 1 BZK 0.000000 2 WZ7 0.000000 3 R_Navy 0.000000 4 H6 0.000000 5 Y-9 0.000000 6 Russia Air 0.000000 給予的樣本預測 Prediction Results: Predicted Location: N Probability of K: 0.05% Probability of N: 99.95% 結論 我們可以看到分類的狀況還蠻好的，但是航母出現的資料數量其實蠻少的，就算全部猜不出現，也能猜對85%，我們看分類出來 第一個是看前五天有沒有出現，再來按照月份、再來是臺灣地區空中動態與航行警告的發布，因此可以知道這幾個因素有相關聯 2023至2024年11月份，航母活動在西太平洋區域而被日方偵測到的天數有113天，未出現的天數為617天，運用機器學習的分類技巧可以預測航母未出現的機率，我們可以觀察到AI在判斷是否有航母出現時首先先檢查前一天是否有航母出現，其次是月份(共軍航母在2023年至2024年通常於9-10月出現比例較大)還有臺灣地區的共機動態(如圖)、航行警告的發布等等都可以作為預判航母出航的徵兆，因此在機器學習技術成熟的今日，在探索軍事動態相關因素上吾人更應該廣泛地蒐集可能原因，然後藉由分析技術預測。\n不過這個模型也只能掌握了某些邏輯，無法100%預測 ","permalink":"https://s0914712.github.io/blog/post-4/","tags":["AI","Military","PLA","軍事動態","航母","深度學習"],"title":"使用機器學習 預測軍事動態-航母篇"},{"categories":["AI解決軍事問題"],"contents":" 使用MLP分析臺海周邊海、空域動態 數據處理在現代資訊系統中至關重要。有效的數據處理能幫助組織從大量數據中提取出有價值的資訊，為後續的決策提供基礎。同時，良好的數據清理、轉換與整合過程能降低錯誤率，增加模型的可靠性。特別是在軍事、金融等高風險領域，數據處理的精確性直接影響任務成敗，以下就數據的清理流程與研究方法進行介紹。\n數據處理 國防部網站上有109年至113年每天的中共解放軍臺海周邊海、空域動態，雖然經過模糊化，但裡面每天出現的機種類型和區域已足夠進行分類和推理，經過資料的整理，統計出大約有20種機型，每天有不一樣的活動動態(如表一)，每一行代表一個時間點的軍事設備部署情況。對於每一行數據，程序創建一個圖，其中節點代表不同類型的軍事設備（如戰鬥機、轟炸機等），節點的值表示該設備的活動的區域代號。我們把它整理成CSV文件，藉由pandas的dataframe函式庫進行分析。\nDATE J10 J11 J16 Y9EW Y8EW Y8ELINT Y8ASW Y8REC Y20 BZK WZ7 Z9 Y9CC SU30 H6 JH7 TB001 CH4 GJ2 KJ500 0601 2 0 0 0 0 0 0 0 0 0 3 0 0 2 0 0 0 0 0 0 0602 0 0 0 0 0 0 0 0 0 0 3 3 0 2 0 0 0 0 0 0 表一：整理後的資料集的一部分\n多層次感知演算法 因為除了空警500外一共有19種機型，並且我們需要知道的是「是否會出現空警500」這個1個答案，因此建立三個輸入層為19個神經節點(用來輸入除空警500外的19種機型出現況況)、隱藏層為3個節點(用來給神經網路交叉比對)，輸出層為1個節點(用來輸出空警500出現機率)的類神經網路， 並定義神經網路間傳遞的方式若數值小於0歸零，數值大於0則為線性之激勵函式（activation function），經過將誤差函數微分並且求極小值的過程後，我們的模型準確度在8成。 如果我們發現某個機型(如轟六)與空警500的關係密切，出現轟六往往會出現空警500，那麼顯示在轟六的權重便會比其他機型高出許多。 初次推理後結果並不如我們預期，因此需要檢討相關變數以提升模型的準確率。\n因此我們嘗試將美艦通過與議員訪臺事件加入分析。 共軍執行任務的理由可能為了例行訓練、演習、或是運用軍事力量威嚇踏入範圍的敵對勢力，例行的訓練或演習很容易有規律的週期，但若是威嚇敵對勢力這種突發事件就沒有所謂的週期，因次我們把歷史上的變因(議員訪臺以及美艦通過臺海)加入模型內，藉以提升模型的準確性。經過增加變數，模型的準確度增加為85.92%。\n組合一 殲10、殲11、運9、運8、BZK無人機、直九直升機、轟六 組合二 殲10、BZK無人機、運9CC、TB001無人機 組合三 排除只出現運8、運8反潛機、運20、彩虹無人機 表二：三種藉由權重分析空警500出現時的搭配機種組合\n使用圖神經網路分析 節點分類 試著試著使用圖神經網路結構來提升訓練 ，首先將資料集內的特徵進行分類，分成無人機(TB001, CH4,GJ2,WZ7, BZK)、戰鬥機(J10,J11,J16,SU30)、偵查任務機(Y9EW, Y8EW,Y8ELINT,Y8ASW, Y8REC,Z9)、以支援機種 (Y20,Y9CC) , C2(KJ500)，其中無人機與偵察任務機擔任集群，保障集群由Y20,Y9CC擔任，突擊集群由H6, JH7擔任，KJ-500擔任指管中心，戰鬥機可能擔任掩護集群以及保障集群之中。\n圖的連接規則： 把圖的連接遵循上述的規則，表示彼此的關係(如圖三)：\nKJ500（預警機）與所有其他節點相連，表示其為指揮中心。 戰鬥機組（J10, J11, J16, SU30）和偵察任務飛機組（Y9EW, Y8ELINT, Y8ASW, Y8REC）只與中心節點（KJ500, H6, JH7, Y20, Y9CC）相連，因偵察任務機組負責提供關鍵資訊給屬於指揮或打擊的機組。 無人機與直升機組（BZK, WZ7, Z9, CH4, TB001）也只與中心節點相連，無人機與直升機負責提供。 中心節點之間互相連接。 如果兩個機種出現的區域類似，也會建立連接。 建立網路模型 有了基本的圖神經網路結構後，我們使用PyTorch Geometric庫實現GNN模型。模型包含兩層圖卷積層（GCNConv）和一個全連接層。模型的輸入是圖的節點特徵和邊的訊息，輸出是一個二元分類（預測KJ500是否出現）。後續再將模型使用Adam優化器進行訓練，計算損失後再進行反向傳播直到損失無法下降為止。\n模型評估：評估指標包括準確率、AUC（曲線下面積）和平均交叉熵。這些指標全面反映了模型的分類性能和預測的確定性。我們定義準確率為： 第一張圖顯示 ROC 曲線（接收者操作特徵曲線），AUC（曲線下的面積）為 1.0，表明模型的分類能力非常好，能夠完美區分正負類別。ROC 曲線越靠近左上角，AUC 越接近 1，說明模型性能越優異。此曲線用於衡量模型的靈敏度與特異度的平衡。 第二張圖是混淆矩陣，用於展示分類結果。圖中顯示模型在 68 個負類別樣本中正確預測了 68 個，有 1 個負類別樣本被錯誤預測為正類別。對於正類別樣本，模型正確預測了 2 個。\n圖:ROC 曲線（接收者操作特徵曲線）\n圖:AUC\n為了比較使用圖神經網路與多層次感知演算法的效能，其中圖神經網路 (GNN) 的準確率最高，達到 0.9859，這意味著它能夠較為準確地預測樣本，而GNN 的 AUC 為 1.0000，這表明它在不同的分類下均有很好的區分能力，具有的平均交叉熵也最低，僅為 0.0654，表明它的預測不確定性最小，並且預測結果更接近真實值，根據這些數據，圖神經網路 (GNN) 在準確率、AUC 和平均交叉熵上均優於多層次感知演算法，顯示出它在此任務上的強大表現。\n模型 圖神經網路 多層次感知 準確率(Accuracy) 98.59% 85.92% 曲線下面積(AUC) 1.0000 0.7681 平均交叉熵(Cross-Entropy) 0.0785 0.3474 表三:準確率 使用「Prophet」預測戰備警巡與西南空域活動 像部隊活動這種多半具有週期性，我們可以結合上述的資料集每天的活動數量與戰備警巡的日期進行進一步的分析，戰備警巡為中共東部戰區位臺灣周邊海空域進行之例行性海空兵力聯合巡邏活動，在戰備警巡實施期間，我們可以由發布的新聞稿中觀察到中共軍機和船艦的活動大幅增加，機型種類也更加完整，中共發言人在例行記者會解釋：「戰備警巡主要目的在進一步提升部隊實戰化訓練水平，增強捍衛國家主權和領土完整的打仗能力，共軍將持續練兵備戰，繼續常態組織有關軍事行動」，而戰備警巡發生的時間往往都在晚上或是清晨，雖然只在我24浬處進行，但軍事行動已具備「猝然攻擊」的戰術戰法，使我國軍最前線海空人員疲於奔命，藉由統計戰備警巡新聞報導的資料，可以大致推估戰備警巡週期大月在6-14天不等，但這僅是藉由統計資料直觀判斷的結果，如果加上深度學習工具，我們更能做到「預測」，我們可以運用由 Facebook 核心科學資料小組。(Facebook Core Data Science Team )發表的「Prophet」程式庫，進行時間序列預測， 「Prophet」程式庫主要是綜合趨勢、季節性、假日或特殊事件等資訊進行預測，被廣泛運用股市或是銷售量預測，經由這套工具分析戰備警巡與時間週期的關係我們可以得到以下結論。 (一)戰備警巡執行週期約在6-14天，發生時間以周三到周五頻率為最高，週六為發生頻率較低的時間： 這也與我們觀察的數據一致，藉由系統得出的執行週期，可推測未來20天哪幾天共軍執行戰備警巡的機率較高(如圖六)。 (二)結合戰備警巡的時間可以提升預測空警500機率的準確度： 我們假設空警500的出現與戰備警巡有很大的關聯性，因此可以將戰備警巡的因素加入評估，可以藉由「Prophet」這套工具來進行分析， 經過分析後可以得知戰備警巡與美艦通過臺海對於空警500的出現有顯著的影響，幾乎每一次中共空軍戰備警巡都會出現空警500，因此若是加入戰備警巡時間與其他機種出現次數作為變量，空警500預測準確度可得到進一步提升(如圖六)\n###運用「Prophet」程式庫預測戰備警巡時間圖\n結論與未來展望 1.研究成果總結本研究透過深度學習技術，成功建立了預測共機活動的模型系統，取得以下具體成果： 建立了具有80%以上準確度的空警500預測模型，能夠根據其他機型組合預測空警500出現機率。 發現特定機型組合(如殲10、殲11、運8電戰、BZK等)與空警500的高度關聯性，並歸納出三種主要的空中編組模式。 運用Prophet工具成功預測戰備警巡週期(6-14天)，並發現其與空警500出現有顯著相關性。 透過區域活動分析，揭示當空警500出現時，其他機型(如運八電偵機、無偵七、轟六)會減少西南部活動而增加東部活動的趨勢。 2 .中共空中作戰訓練模式反映了其明確的戰略布局。在西南空域的平時演練與演訓中，空中預警機擔任前線指揮中心，電偵機和警戒機作為先遣部隊，為後方轟六轟炸機的攻擊行動提供掩護，而圖神經網路技術則為我們提供這些空中陣形的可能性。習近平提出的「實戰化訓練」亦即「理念仗在哪裡打，兵就在哪裡練」。海空聯合遠程飛行訓練正是這一戰略思維的具體實踐。從訓練方式和目標來看，美軍可能是其主要假想敵，而臺灣的西南空域則是其練兵場所。東部戰區與南部站區於此空域部署同等規模兵力進行區域接力監視，空警500(KJ-500)則扮演中繼與指揮的角色。\n","permalink":"https://s0914712.github.io/blog/post-1/","tags":["AI","Military","PLA","軍事動態","軍演","深度學習"],"title":"使用MLP分析臺海周邊海、空域動態"},{"categories":["AI解決軍事問題"],"contents":" 認識 Prophet Prophet 是一個由 Facebook Core Data Science Team 發表的開源代碼庫，用於時間序列預測，基於 Python 和 R 語言。相較於自行訓練時間序列預測模型，Prophet 的一些優點如下：‌\n改善模型選擇和調參的時間成本：時間序列有許多經典算法如 AR, VAR, ARMA, ARIMA, 指數平滑法等，選擇模型和調參的過程可被自動化。 提供讓分析師、領域專家能根據經驗法則設定的參數：例如歷史週期、特殊節日的日期等，不會因為寫成制式套件就失去自己手刻的好處。\n安裝步驟 (0) 首先建議要用 conda 創建虛擬環境，避免不同套件版本產生不必要的衝突\n(base) $ conda create -n env_name python=3.7 (base) $ conda activate env_name (env_name) $ Windows 而 Windows 的預設編譯器是 MSVC ，pystan 並不支援，因此需要額外安裝 Windows 版本的 gcc：mingw-w64 compiler\n(env_name) $ conda install libpython m2w64-toolchain -c msys2 確認都有安裝好支援的 c++ compiler 後，記得先安裝 pystan 。以下都建議使用 conda install，不要用 pip install，虛擬環境下的 jupyter notebook 可能會 import 不到。\n(env_name) $ conda install -c pystan conda-forge 接著安裝 prophet （注意：v1.0 後的版本套件名稱為 prophet，不再是 fbprophet，網路上很多教學文章還以舊稱，記得在 import 時調整即可\n(env_name) $ conda install -c prophet conda-forge ###完成好以後就可以載入資料集進行分析\n像部隊活動這種多半具有週期性，我們可以結合上述的資料集每天的活動數量與戰備警巡的日期進行進一步的分析，戰備警巡為中共東部戰區位臺灣周邊海空域進行之例行性海空兵力聯合巡邏活動，在戰備警巡實施期間，我們可以由發布的新聞稿中觀察到中共軍機和船艦的活動大幅增加，機型種類也更加完整，中共發言人在例行記者會解釋： 「戰備警巡主要目的在進一步提升部隊實戰化訓練水平，增強捍衛國家主權和領土完整的打仗能力，共軍將持續練兵備戰，繼續常態組織有關軍事行動」\n，而戰備警巡發生的時間往往都在晚上或是清晨，雖然只在我24浬處進行，但軍事行動已具備「猝然攻擊」的戰術戰法，使我國軍最前線海空人員疲於奔命，藉由統計戰備警巡新聞報導的資料，可以大致推估戰備警巡週期大月在6-14天不等，但這僅是藉由統計資料直觀判斷的結果，如果加上深度學習工具，我們更能做到「預測」，「Prophet」程式庫，進行時間序列預測\n# 將 day.csv 匯入資料框 # 由 parse_dates 指定代表日期的行 df = pd.read_csv(\u0026#39;day.csv\u0026#39;, parse_dates=[1]) df2 = pd.read_csv(\u0026#39;/content/dateAndcharacter.csv\u0026#39;,parse_dates=[1]) DF_PLANE = pd.DataFrame(df2) DF_PLANE[\u0026#39;DATE\u0026#39;] = pd.to_datetime(DF_PLANE[\u0026#39;DATE\u0026#39;], format=\u0026#39;%Y%m%d\u0026#39;) DF_PLANE 以上的資料集為2023年國防部軍事動態網站上擷取出來的公開資料 我把它彙整成csv檔案 連結在這裡 2023年國防部軍事動態網站軍事動態彙整\n接下來就開始 開工寫程式 載入資料集 df2 = df.copy() DF_PLANE2=DF_PLANE.copy() DF_PLANE2=DF_PLANE2[[\u0026#39;DATE\u0026#39;, \u0026#39;KJ-500\u0026#39;]] #把想要評估的對象設定為y 這邊以KJ-500 DF_PLANE2.columns = [\u0026#39;ds\u0026#39;, \u0026#39;y\u0026#39;] # 設定分割日 mday mday = pd.to_datetime(\u0026#39;2023-10-1\u0026#39;) # 建立訓練用 index 與驗證用 index train_index = DF_PLANE2[\u0026#39;ds\u0026#39;] \u0026lt; mday test_index = DF_PLANE2[\u0026#39;ds\u0026#39;] \u0026gt;= mday # 分割輸入資料 x_train = DF_PLANE2[train_index] x_test = DF_PLANE2[test_index] # 分割日期資料（用於繪製圖形） dates_test = DF_PLANE2[\u0026#39;ds\u0026#39;][test_index] 選擇prophet演算法 from prophet import Prophet # 選擇模型 # 這 3 個 seasonality 參數的設定很重要 # 本資料為日單位，因此不需使用 daily_seasonality # weekly_seasonality 與 daily_seasonality 除了 True/False 以外， # 也可以指定成數值（三角函數的數量） # seasonality_mode: additive(預設) multiplicative m1 = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True,interval_width=0.9, seasonality_mode=\u0026#39;multiplicative\u0026#39;) 訓練與預測 m1.fit(x_train) future1 = m1.make_future_dataframe(periods=61, freq=\u0026#39;D\u0026#39;) 確認結果 display(future1.head()) display(future1.tail()) fcst1 = m1.predict(future1) fig = m1.plot_components(fcst1) plt.show() 繪製圖形 計算與實際的差異值(R2) # 只從 fcst2 中提取預測部分 ypred2 = fcst2[-88:][[\u0026#39;yhat\u0026#39;]].values # 計算 R2 值 score2 = r2_score(ytest1, ypred2) #### 確認結果 r2_text2 = f\u0026#39;R2 score:{score2:.4f}\u0026#39; print(r2_text2) 繪製時間序列圖 import matplotlib.dates as mdates fig, ax = plt.subplots(figsize=(8, 4)) #### 繪製圖形 ax.plot(dates_test, ytest1, label=\u0026#39;標準答案\u0026#39;, c=\u0026#39;k\u0026#39;) ax.plot(dates_test, ypred1, label=\u0026#39;預測結果 v1\u0026#39;, c=\u0026#39;c\u0026#39;) ax.plot(dates_test, ypred2, label=\u0026#39;預測結果 v2\u0026#39;, c=\u0026#39;b\u0026#39;) # 日期刻度間隔 # 於每週四顯示日期 weeks = mdates.WeekdayLocator(byweekday=mdates.TH) ax.xaxis.set_major_locator(weeks) #### 將日期刻度標籤文字旋轉 90 度 ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=90) #### 開始日與結束日 sday = pd.to_datetime(\u0026#39;2023-10-1\u0026#39;) eday = pd.to_datetime(\u0026#39;2023-12-31\u0026#39;) ax.set_xlim(sday, eday) #### 顯示網格等 ax.grid() ax.legend() ax.set_title(\u0026#39;KJ-500 出現機率預測結果 \u0026#39; + r2_text2) 輸出畫面 結論 經由這套工具分析戰備警巡與時間週期的關係我們可以得到以下結論。 (一)戰備警巡執行週期約在6-14天，發生時間以周三到周五頻率為最高，週六為發生頻率較低的時間： 這也與我們觀察的數據一致，藉由系統得出的執行週期，可推測未來20天哪幾天共軍執行戰備警巡的機率較高\n","permalink":"https://s0914712.github.io/blog/post-3/","tags":["AI","Military","PLA","軍事動態","軍演","深度學習"],"title":"使用機器學習 預測軍事動態-西南空域篇"},{"categories":["HTML \u0026 CSS"],"contents":"Emphasis 認識 Prophet Prophet 是一個由 Facebook Core Data Science Team 發表的開源代碼庫，用於時間序列預測，基於 Python 和 R 語言。相較於自行訓練時間序列預測模型，Prophet 的一些優點如下：‌\n改善模型選擇和調參的時間成本：時間序列有許多經典算法如 AR, VAR, ARMA, ARIMA, 指數平滑法等，選擇模型和調參的過程可被自動化。 提供讓分析師、領域專家能根據經驗法則設定的參數：例如歷史週期、特殊節日的日期等，不會因為寫成制式套件就失去自己手刻的好處。\n安裝步驟 (0) 首先建議要用 conda 創建虛擬環境，避免不同套件版本產生不必要的衝突\n(base) $ conda create -n env_name python=3.7 (base) $ conda activate env_name (env_name) $ Windows 而 Windows 的預設編譯器是 MSVC ，pystan 並不支援，因此需要額外安裝 Windows 版本的 gcc：mingw-w64 compiler\n(env_name) $ conda install libpython m2w64-toolchain -c msys2 確認都有安裝好支援的 c++ compiler 後，記得先安裝 pystan 。以下都建議使用 conda install，不要用 pip install，虛擬環境下的 jupyter notebook 可能會 import 不到。\n(env_name) $ conda install -c pystan conda-forge 接著安裝 prophet （注意：v1.0 後的版本套件名稱為 prophet，不再是 fbprophet，網路上很多教學文章還以舊稱，記得在 import 時調整即可\n(env_name) $ conda install -c prophet conda-forge 完成好以後就可以載入資料集進行分析 像部隊活動這種多半具有週期性，我們可以結合上述的資料集每天的活動數量與戰備警巡的日期進行進一步的分析，戰備警巡為中共東部戰區位臺灣周邊海空域進行之例行性海空兵力聯合巡邏活動，在戰備警巡實施期間，我們可以由發布的新聞稿中觀察到中共軍機和船艦的活動大幅增加，機型種類也更加完整，中共發言人在例行記者會解釋：「戰備警巡主要目的在進一步提升部隊實戰化訓練水平，增強捍衛國家主權和領土完整的打仗能力，共軍將持續練兵備戰，繼續常態組織有關軍事行動」，而戰備警巡發生的時間往往都在晚上或是清晨，雖然只在我24浬處進行，但軍事行動已具備「猝然攻擊」的戰術戰法，使我國軍最前線海空人員疲於奔命，藉由統計戰備警巡新聞報導的資料，可以大致推估戰備警巡週期大月在6-14天不等，但這僅是藉由統計資料直觀判斷的結果，如果加上深度學習工具，我們更能做到「預測」，「Prophet」程式庫，進行時間序列預測\n# 將 day.csv 匯入資料框 # 由 parse_dates 指定代表日期的行 df = pd.read_csv(\u0026#39;day.csv\u0026#39;, parse_dates=[1]) df2 = pd.read_csv(\u0026#39;/content/dateAndcharacter.csv\u0026#39;,parse_dates=[1]) DF_PLANE = pd.DataFrame(df2) DF_PLANE[\u0026#39;DATE\u0026#39;] = pd.to_datetime(DF_PLANE[\u0026#39;DATE\u0026#39;], format=\u0026#39;%Y%m%d\u0026#39;) DF_PLANE df2 = df.copy() DF_PLANE2=DF_PLANE.copy() DF_PLANE2=DF_PLANE2[[\u0026#39;DATE\u0026#39;, \u0026#39;KJ-500\u0026#39;]] #把想要評估的對象設定為y 這邊以KJ-500 DF_PLANE2.columns = [\u0026#39;ds\u0026#39;, \u0026#39;y\u0026#39;] # 設定分割日 mday mday = pd.to_datetime(\u0026#39;2023-10-1\u0026#39;) # 建立訓練用 index 與驗證用 index train_index = DF_PLANE2[\u0026#39;ds\u0026#39;] \u0026lt; mday test_index = DF_PLANE2[\u0026#39;ds\u0026#39;] \u0026gt;= mday # 分割輸入資料 x_train = DF_PLANE2[train_index] x_test = DF_PLANE2[test_index] # 分割日期資料（用於繪製圖形） dates_test = DF_PLANE2[\u0026#39;ds\u0026#39;][test_index] ##選擇演算法\nfrom prophet import Prophet # 選擇模型 # 這 3 個 seasonality 參數的設定很重要 # 本資料為日單位，因此不需使用 daily_seasonality # weekly_seasonality 與 daily_seasonality 除了 True/False 以外， # 也可以指定成數值（三角函數的數量） # seasonality_mode: additive(預設) multiplicative m1 = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True,interval_width=0.9, seasonality_mode=\u0026#39;multiplicative\u0026#39;) ##訓練與預測\nm1.fit(x_train) future1 = m1.make_future_dataframe(periods=61, freq=\u0026#39;D\u0026#39;) 確認結果 display(future1.head()) display(future1.tail()) fcst1 = m1.predict(future1) fig = m1.plot_components(fcst1) plt.show() #繪製圖形\n計算與實際的差異值(R2) # 只從 fcst2 中提取預測部分 ypred2 = fcst2[-88:][[\u0026#39;yhat\u0026#39;]].values # 計算 R2 值 score2 = r2_score(ytest1, ypred2) # 確認結果 r2_text2 = f\u0026#39;R2 score:{score2:.4f}\u0026#39; print(r2_text2) 繪製時間序列圖 import matplotlib.dates as mdates fig, ax = plt.subplots(figsize=(8, 4)) # 繪製圖形 ax.plot(dates_test, ytest1, label=\u0026#39;標準答案\u0026#39;, c=\u0026#39;k\u0026#39;) ax.plot(dates_test, ypred1, label=\u0026#39;預測結果 v1\u0026#39;, c=\u0026#39;c\u0026#39;) ax.plot(dates_test, ypred2, label=\u0026#39;預測結果 v2\u0026#39;, c=\u0026#39;b\u0026#39;) # 日期刻度間隔 # 於每週四顯示日期 weeks = mdates.WeekdayLocator(byweekday=mdates.TH) ax.xaxis.set_major_locator(weeks) # 將日期刻度標籤文字旋轉 90 度 ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=90) # 開始日與結束日 sday = pd.to_datetime(\u0026#39;2023-10-1\u0026#39;) eday = pd.to_datetime(\u0026#39;2023-12-31\u0026#39;) ax.set_xlim(sday, eday) # 顯示網格等 ax.grid() ax.legend() ax.set_title(\u0026#39;KJ-500 出現機率預測結果 \u0026#39; + r2_text2) 輸出畫面 ","permalink":"https://s0914712.github.io/blog/post-2/","tags":["Ai解決軍事問題","Game","React","Python","New"],"title":"使用Prophet 預測週期方法"}]